
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["daniel_antal"],"categories":null,"content":"Daniel Antal is an experienced data scientist, consultant, economist, and the co-founder of Reprex, a Netherlands-based startup that brings the benefits of big data to small organizations with shared resources and research automation. He applies data science practice, open-source software development with sound economics and valuation techniques.\nHe is also a research affiliate at the [Centre for Competition Policy](https://www.linkedin.com/school/ccpuea/ and at the Institute for Information Law of the University of Amsterdam.\n","date":1656486720,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656486720,"objectID":"0d25123d0ac03749a9d015a7325a60af","permalink":"https://competition.dataobservatory.eu/authors/daniel_antal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel_antal/","section":"authors","summary":"Daniel Antal is an experienced data scientist, consultant, economist, and the co-founder of Reprex, a Netherlands-based startup that brings the benefits of big data to small organizations with shared resources and research automation.","tags":null,"title":"Daniel Antal","type":"authors"},{"authors":["kasia_kulma"],"categories":null,"content":"Kasia is an experienced data scientist consultant and open-source contributor. She focuses her efforts on bringing together open-source technology and data in fighting climate change and speeding up the race to net-zero.\n","date":1623844800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623844800,"objectID":"0afdeed043f12908a06671e99c798613","permalink":"https://competition.dataobservatory.eu/authors/kasia_kulma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kasia_kulma/","section":"authors","summary":"Kasia is an experienced data scientist consultant and open-source contributor. She focuses her efforts on bringing together open-source technology and data in fighting climate change and speeding up the race to net-zero.","tags":null,"title":"Kasia Kulma","type":"authors"},{"authors":["leo_lahti"],"categories":null,"content":"Leo Lahti is associate professor in data science at the University of Turku, and co-founder of rOpenGov, an open developer network that generates reproducible research tools for open government data analytics in R. The twenty years of work experience in data-intensive applications of computational and data sciences and machine learning, ranging from natural sciences to the humanities, help to bridge the gap between the theory and applications in quantitative research.\n","date":1623844800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623844800,"objectID":"a1d5247c1a2ffd5dab382f767c1cb2d0","permalink":"https://competition.dataobservatory.eu/authors/leo_lahti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leo_lahti/","section":"authors","summary":"Leo Lahti is associate professor in data science at the University of Turku, and co-founder of rOpenGov, an open developer network that generates reproducible research tools for open government data analytics in R.","tags":null,"title":"Leo Lahti","type":"authors"},{"authors":["rOpenGov"],"categories":null,"content":"rOpenGov is an international open source developer network for open government data analytics in R and has released various packages for retrieval, refinement, and analysis of open data from statistical authorities over the past decade. The network was formally introduced at the NIPS Machine Learning Open Source Software workshop in 2013, and has now active contributors from multiple countries. The project aims to facilitate the seamless incorporation of open government data into reproducible statistical and probabilistic programming workflows.\n","date":1623844800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623844800,"objectID":"1c30eb6dcd447650988eddc5a6b16517","permalink":"https://competition.dataobservatory.eu/authors/ropengov/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ropengov/","section":"authors","summary":"rOpenGov is an international open source developer network for open government data analytics in R and has released various packages for retrieval, refinement, and analysis of open data from statistical authorities over the past decade.","tags":null,"title":"rOpenGov","type":"authors"},{"authors":["annette_wong"],"categories":null,"content":"Annette is a digital strategist and product marketer with experience working with startup tech companies to develop tailored marketing and communication programs to win new business. She is passionate about data science, creative industries, and emerging technology.\n","date":1623240000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623240000,"objectID":"7a3535792e35bfe298852f1b2823b341","permalink":"https://competition.dataobservatory.eu/authors/annette_wong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/annette_wong/","section":"authors","summary":"Annette is a digital strategist and product marketer with experience working with startup tech companies to develop tailored marketing and communication programs to win new business. She is passionate about data science, creative industries, and emerging technology.","tags":null,"title":"Annette Wong","type":"authors"},{"authors":["karel_volckaert"],"categories":null,"content":"Karel Volckaert is a civil engineer, financial expert and consultant. He is contributing with policy use cases and policy analysis based on our maps and datasets. Read Karel’s introdcution, Credibility is Enhanced Through Cross Links Between Different Data from Different Domains, here.\n","date":1623178200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623178200,"objectID":"59f786a3960f56e706ffcd4d13b98195","permalink":"https://competition.dataobservatory.eu/authors/karel_volckaert/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karel_volckaert/","section":"authors","summary":"Karel Volckaert is a civil engineer, financial expert and consultant. He is contributing with policy use cases and policy analysis based on our maps and datasets. Read Karel’s introdcution, Credibility is Enhanced Through Cross Links Between Different Data from Different Domains, here.","tags":null,"title":"Karel Volkaert","type":"authors"},{"authors":["suzan_sidal"],"categories":null,"content":"Suzan is a security consultant working on digitalisation, good governance and related data strategies in the European Union. She is a data enthusiast and advocates for more accountability and responsibility of open data usage in public administration. Read her, We Need More Reliable Datasets on the Urban Heat Resilience and Disaster Risk Reduction, here\n","date":1623096000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623096000,"objectID":"640057d612f15421a2134d02b54d6f6b","permalink":"https://competition.dataobservatory.eu/authors/suzan_sidal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/suzan_sidal/","section":"authors","summary":"Suzan is a security consultant working on digitalisation, good governance and related data strategies in the European Union. She is a data enthusiast and advocates for more accountability and responsibility of open data usage in public administration.","tags":null,"title":"Suzan Sidal","type":"authors"},{"authors":["pyry_kantanen"],"categories":null,"content":"Pyry Kantanen is a social science student and R developer. He is contributing with data curation and R package testing. Read his introduction, Comparing Data to Oil is a Cliché: Crude Oil Has to Go Through a Number of Steps and Pipes Before it Becomes Useful, here.\n","date":1623060000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623060000,"objectID":"e0cd12337521fee32275a40c6376d806","permalink":"https://competition.dataobservatory.eu/authors/pyry_kantanen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pyry_kantanen/","section":"authors","summary":"Pyry Kantanen is a social science student and R developer. He is contributing with data curation and R package testing. Read his introduction, Comparing Data to Oil is a Cliché: Crude Oil Has to Go Through a Number of Steps and Pipes Before it Becomes Useful, here.","tags":null,"title":"Pyry Kantanen","type":"authors"},{"authors":["peter_ormosi"],"categories":null,"content":"Peter is a competition economist and legal scholar, and an expert on innovation with an interest in the music industry, particularly streaming. He is curating data sets for our observatory that are building bridges between patent and intellectual property information, connect antitrust and economic activity (NACE markets), and generally help understanding how big data and AI is shaping competition and market concentration in the European economy. Read his introduction here.\n","date":1622653200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622653200,"objectID":"37c064fe86d74114064ba2099cf92057","permalink":"https://competition.dataobservatory.eu/authors/peter_ormosi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/peter_ormosi/","section":"authors","summary":"Peter is a competition economist and legal scholar, and an expert on innovation with an interest in the music industry, particularly streaming. He is curating data sets for our observatory that are building bridges between patent and intellectual property information, connect antitrust and economic activity (NACE markets), and generally help understanding how big data and AI is shaping competition and market concentration in the European economy.","tags":null,"title":"Peter Ormosi","type":"authors"},{"authors":["botond_vitos"],"categories":null,"content":"Botond is a data scientist and cultural studies scholar with an interest in digital humanities, music research and festival cultures. His past projects were focused on grassroots music scenes and alternative communities, and he is currently contributing to the development of our Digital Music Observatory and Listen Local initiatives. He is the production editor and art director of the peer reviewed journal Dancecult. He is taking care of our Data APIs and “big data” collection. Read his introduction here.\n","date":1622545200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625655600,"objectID":"c1536294e7c1b417b634c0a8cbd81ce6","permalink":"https://competition.dataobservatory.eu/authors/botond_vitos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/botond_vitos/","section":"authors","summary":"Botond is a data scientist and cultural studies scholar with an interest in digital humanities, music research and festival cultures. His past projects were focused on grassroots music scenes and alternative communities, and he is currently contributing to the development of our Digital Music Observatory and Listen Local initiatives.","tags":null,"title":"Botond Vitos","type":"authors"},{"authors":null,"categories":null,"content":"Our Competition Data Observatory is a fully automated, open source, open data observatory that produces new indicators from open data sources and experimental big data sources, with authoritative copies and a modern API.\nOur observatory is monitoring the certain segments of the European economy, and develops tools for computational antitrust in Europe. We take a critical SME-, intellectual property policy and competition policy point of view automation, robotization, and the AI revolution on the service-oriented European social market economy.\nWe would like to create early-warning, risk, economic effect, and impact indicators that can be used in scientific, business and policy contexts for professionals who are working on re-setting the European economy after a devastating pandemic and in the age of AI. We would like to map data between economic activities (NACE), antitrust markets, and sub-national, regional, metropolitian area data.\nGet involved in services: our ongoing projects, team of contributors, open-source libraries and use our data for publications. See some use cases.\nFollow news about us or the more comprehensive Data \u0026amp; Lyrics blog.\nContact us .\nDownload our competition presentation\nOur Product/Market Fit was validated in the world’s 2nd ranked university-backed incubator program, the Yes!Delft AI Validation Lab.\n","date":1615852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615852800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://competition.dataobservatory.eu/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Our Competition Data Observatory is a fully automated, open source, open data observatory that produces new indicators from open data sources and experimental big data sources, with authoritative copies and a modern API.","tags":null,"title":"Competition Data Observatory","type":"authors"},{"authors":["line"],"categories":null,"content":"Andrés is a data scientist and an ethnomusicologist. He is an experienced international researcher with interests that lie at the intersection between data science and the humanities. He helps our team with quantifying (ethno-)musicological variables to train better AI models and to make our Demo Music Observatory more valuable for its users. He also helps us reach out to potential partners in North America.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e868dbbe6f9fb7afda2116432b74ef6","permalink":"https://competition.dataobservatory.eu/authors/andres/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andres/","section":"authors","summary":"Andrés is a data scientist and an ethnomusicologist. He is an experienced international researcher with interests that lie at the intersection between data science and the humanities. He helps our team with quantifying (ethno-)musicological variables to train better AI models and to make our Demo Music Observatory more valuable for its users.","tags":null,"title":"Andrés García Molina, PhD","type":"authors"},{"authors":["datagraver"],"categories":null,"content":"Datagraver combines IT experience with the knowledge of finding and opening up various data sources, mainly in population development, security, climate, energy and politics, and has a track record to turn data into information for media, charities and interest groups, and various business and policy organizations. Datagraver is a well-known source in Dutch data journalism and used to have a weekly column on an RTL television channel. Datagraver is founded by Stephan Okhuijsen.\nSee for example: Well-known Climate Data Sets Visualizatd \u0026amp; Always Updated\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a0c64dce1054d901e4e945e1ec4c7c9","permalink":"https://competition.dataobservatory.eu/authors/datagraver/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/datagraver/","section":"authors","summary":"Datagraver combines IT experience with the knowledge of finding and opening up various data sources, mainly in population development, security, climate, energy and politics, and has a track record to turn data into information for media, charities and interest groups, and various business and policy organizations.","tags":null,"title":"Datagraver","type":"authors"},{"authors":["curator"],"categories":null,"content":"Help us curate data – tell us what sort of information is missing from your research agenda. Challenge us and collaborate with us in the crafting of valuable datasets that combine domain knowledge with reproducible, open data research practices.\nCuratorial Expectations Your first contribution can be made without writing a single program code – but if you are experienced in reproducible science, than you can also submit a code that creates your data.\nGreat new data products are always made out of some relevant, professional, often playful curiosity to our topics. If you have that curiosity in the field of economic policies, particularly in computational antitrust, innovation research, and understanding the statistically under-represented micro- and small enterprises, join our Economy Data Observatory curatorial team. If you have an interest in environmental research of climate change, designing urban, social and economic mitigation strategies, or undertanding how people think about climate change, join our Green Deal Data Observatory curatorial team. If you are interested in music, musicians, or etchical, trustworthy AI and data governance issues, join our Digital Music Observatory curatorial team. This last observatory is dealing with data governance issues, too, showcase experience from the music industry.\nGet inspired\nSend us a plain language document, preferably in any flavor of markdown (See subchapter @ref(markdown) in the Tools), or even in a clear text email about the indicator. What should the indicator be used for, how it should be measured, with what frequency, and what could be the open data source to acquire the observations. Experienced data scientists can send us a Jupiter Notebook or an Rmarkdown file with code, but this submission can be a simple plain language document without numbers.\nSometimes we put our hands on data that looks like a unique starting point to create a new indicator. But our indicator will be flawed, if the original dataset is flawed. And it can be flawed in many ways, most likely that some important aspect of the information was omitted, or the data is autoselected, for example, under-sampling women, people of color, or observations from small or less developed countries. See our curatorial handbook on how to remain critical.\nExperienced programmers are welcome to participate in our developer team, and become contributors, or eventually co-authors of the (scientific) software codes that we make to continuously improve our data observatories. All our data code is open source. At this level, you are expected to be able to raise and/or pick up and solve an issue in our observatory’s Github repository, or its connecting statistical repositories.\nTechnical Requirements Make sure that you read the Contributors Covenant. You must make this pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. Participating in our data observatories requires everybody to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. It’s better this way for you and for us!\nMake sure that you have and ORCiD ID. This is a standard identification for scientific publications. We need your numeric ORCiD ID.\nMake sure that you have a Zenodo account which is connected to your ORCiD ID. This enables you to publish data under your name. If you curate data for our observatories, you will be the indicator’s first author, and depending on what processes help you, the author of the (scientific) code that helps you calculate the values will be your co-author.\nIf you write code in R or Python, connect to us via Github.\nSend us this text file with your biography elements. Best if you fill it out with Notepad, TextEdit, Vim or other clean text editor, but you can also send it back as an rtf file.\nIf you feel you need chatting on onboarding, contact us on Keybase - it’s lightweight, discrete, encrypted, your mother, partner and friends are not there, it is free, open source, and can share/exchange files, too. Otherwise in email.\nMore about contributing: Automated Observatory Contributors’ Handbook.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dd7e87cb618c3c88b40f53ca6635c9ec","permalink":"https://competition.dataobservatory.eu/authors/curator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/curator/","section":"authors","summary":"Help us curate data – tell us what sort of information is missing from your research agenda. Challenge us and collaborate with us in the crafting of valuable datasets that combine domain knowledge with reproducible, open data research practices.","tags":null,"title":"New Curators","type":"authors"},{"authors":["developer"],"categories":null,"content":"Our observatory is currently technically maintained by Reprex BV, an early stage Dutch-American data science startup specialized on open collaborations with open data and open-source data solutions. We are looking for contributor, either as recognized, unpaid, open-source contributors, or as granted and paid contributors when funding available.\nR developer(s) We are looking for intermediate or advanced R users with a passion for open data and open science for the maintenance of our CRAN-released R packages and the development of further packages. We are pursuing a hybrid model, providing the R community with open-source packages, and engaging in paid work that utilizes this software in commercial or academic environments.\nOur ideal candidate(s) are a) at least intermediate-level R programmers or possess domain-specific knowledge relevant to our packages, or\nb) advanced in R programming and agnostic to actual packages\nc) excited to maintain and develop one or more of our packages\nAll of our packages follow the modernization of the R language and are built on rlang and vctrs. All the packages use the tidyverse as a dependency, which creates a consistent user interface (i.e. dplyr, tidyr, tidyselect.)\nPackages -iotables: an R package for reproducible input-output analysis, economic, and environmental impact assessment. The domain specific knowledge is input-output economics, multiplier analysis, and environmental impact analysis. A working knowledge of SNA or an interest in macro-finance is a plus. We develop this application within the rOpenGov community and the rOpenSci community. The application has various uses in banking, insurance, music industry, and policy design.\nretroharmonize: an R package for retrospective survey harmonization and survey recycling. The domain specific knowledge is an interest in international, multi-language surveys, longitudinal surveys, and the reuse of survey data. We develop this application within the rOpenGov community and the rOpenSci community. The application has various uses in survey harmonization, data integration, and survey design.\nregions: an R package for adjusting sub-national boundaries for the making of regional statistics. While the U.S. has relatively stable sub-national boundaries (the US postal codes), most nations change their internal boundaries very frequently. Currently, regions tracks these changes in Europe, but our package could and should be extended to all ISO-conforming sub-national boundaries globally. An ideal domain-specific interest is geography, cartography, and/or small-area statistics. The package is currently not developed actively, but we expect it to be developed in a small-area statistics context, or for surveying withing a regional component.\nWe are also contributing to a range of packages relevant for music analysis, open data access and open science data access and we are planning the release of new open source and non-open-source products.\nWe are looking for individual(s) who can resolve issues via Github. Time commitments are flexible and compensation is commensurate with experience and skill.\nShiny developer We are looking for a contract-based Shiny developer who can create engaging, user-friendly multi-language Shiny interfaces to our R products. We are interested in working with candidates with experience in Shiny development and/or deployment skills, in particular, the ability to dockerize and deploy in the cloud. Currently we deploy on AWS and Netlify, but potentially we may need to deploy on other cloud servers.\nOur Shiny applications have multiple users:\nMusic organizations and music researchers connected to our Digital Music Observatory Sustainable finance, sustainable reporting, and climate mitigation policy experts related to our Green Deal Data Observatory Antitrust experts, antitrust authorities, and merger analysts associated with our Competition Data Observatory Various creative industry stakeholders related to our Cultural and Creative Sectors Industries Data Observatory, mainly related to book publishing and film production. Some of our applications are expected to be able to communicate with various Rest APIs, e.g.: the Eurostat and Spotify Rest APIs. Our applications must work with several language; buttons, alternate texts, and descriptions must be parameterized and available for localization. The visual elements must follow simple visual structures and a unified colour palette.\nGet in touch with us on Keybase or in email.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"25c8c4ca7d3bdc3f31d6afce9bdb583a","permalink":"https://competition.dataobservatory.eu/authors/developer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/developer/","section":"authors","summary":"Our observatory is currently technically maintained by Reprex BV, an early stage Dutch-American data science startup specialized on open collaborations with open data and open-source data solutions. We are looking for contributor, either as recognized, unpaid, open-source contributors, or as granted and paid contributors when funding available.","tags":null,"title":"New Developers","type":"authors"},{"authors":["team"],"categories":null,"content":"Our observatories are managed by Reprex, a Dutch open data and AI startup. Our observatories are developed in an open collaboration with developers and curators, and a team of devoted service developers who hope that our observatories will remain both technically and financially feasible. If you would like to involved in the business feasibility, we are open to increasing the range of capabilities that our team encompasses, from communications and marketing to human resources, data analysis, and research \u0026amp; development. Feel free to drop us a line and propose your own way of collaborating.\nPassion About Our Topic You are passionate about one of our topics, but you do not feel that you have the skills yet to become a data curator or a developer. You have a curiosity in the field of economic policies, particularly in computational antitrust, innovation research, and understanding the statistically under-represented micro- and small enterprises, join our Economy Data Observatory as a volunteer. You are passionate about environmental research of climate change, designing urban, social and economic mitigation strategies, or undertanding how people think about climate change, join our Green Deal Data Observatory team as a volunteer. You want to know how musicians can make a living after the pandemic? How can we make sure that music made by womxn, small country artists or artists of color gets and equal chance? Are you interested in the future of ethical AI and data governance? Join our Digital Music Observatory team as a volunteer. Get inspired\nPassion For Trustworthy AI, Open Data and Open Source Projects You want to learn to write scientifically valid, open source code in R or Python, but you are a beginner. We help you anywhere - you can even start copyediting or technical documentation (it is a must in open source development) and create tutorials for you to interact with our data and products (if if helps you, it will help others.)\nAs a business economist or legal professional, you are interested how open data, open source software, research automation and ethical, trustworthy AI products can find their market.\nAs a blogger, data journalist or marketeer you would like to help to make open data, and transparent, ethical, open AI more widely known and used.\nTechnical Requirements Make sure that you read the Contributors Covenant. You must make this pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. Participating in our data observatories requires everybody to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. It’s better this way for you and for us!\nSend us this text file with your biography elements. Best if you fill it out with Notepad, TextEdit, Vim or other clean text editor, but you can also send it back as an rtf file.\nIf you feel you need chatting on onboarding, contact us on Keybase - it’s lightweight, discrete, encrypted, your mother, partner and friends are not there, it is free, open source, and can share/exchange files, too. Otherwise in email.\nMore about contributing: Automated Observatory Contributors’ Handbook.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3727ee9a01674e6999f24524b05a5897","permalink":"https://competition.dataobservatory.eu/authors/team/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/team/","section":"authors","summary":"Our observatories are managed by Reprex, a Dutch open data and AI startup. Our observatories are developed in an open collaboration with developers and curators, and a team of devoted service developers who hope that our observatories will remain both technically and financially feasible.","tags":null,"title":"Observatory Business Associate","type":"authors"},{"authors":["robin_nagy"],"categories":null,"content":"Robin is a freelancer working on projects using emerging technologies. Main areas of experties are Business Development, Project Management, Innovation, Governance and Corporate culture. Recent interest: Exponential technologies, Exponential business.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a40b86173def842e936c920101754980","permalink":"https://competition.dataobservatory.eu/authors/robin_nagy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/robin_nagy/","section":"authors","summary":"Robin is a freelancer working on projects using emerging technologies. Main areas of experties are Business Development, Project Management, Innovation, Governance and Corporate culture. Recent interest: Exponential technologies, Exponential business.","tags":null,"title":"Robin Nagy","type":"authors"},{"authors":["stephan_okhuijsen"],"categories":null,"content":"Stephan Okhuijsen is the founder of Datagraver, a small company that uses (open) data to inform a wider audience, sometimes through media, and helps organizations using (open) data to create valuable information. He has over 30 years of experience in IT and has been active for over 15 years in the open data movement in the Netherlands. As Datagraver he created several climate related vizualisations that have been picked up by some large media outlets all over the world.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5352c6af856312e03e132bda49f50a79","permalink":"https://competition.dataobservatory.eu/authors/stephan_okhuijsen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stephan_okhuijsen/","section":"authors","summary":"Stephan Okhuijsen is the founder of Datagraver, a small company that uses (open) data to inform a wider audience, sometimes through media, and helps organizations using (open) data to create valuable information.","tags":null,"title":"Stephan Okhuijsen","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://competition.dataobservatory.eu/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Daniel Antal"],"categories":null,"content":"Interoperable, FAIR datasets The primary aim of dataset is create well-referenced, well-described, interoperable datasets from data.frames, tibbles or data.tables that translate well into the W3C DataSet definition within the Data Cube Vocabulary in a reproducible manner. The data cube model in itself is is originated in the Statistical Data and Metadata eXchange, and it is almost fully harmonzied with the Resource Description Framework (RDF), the standard model for data interchange on the web[^1].\nA mapping of R objects into these models has numerous advantages:\nMakes data importing easier and less error-prone; Leaves plenty of room for documentation automation, resulting in far better reusability and reproducability; The publication of results from R following the FAIR principles is far easier, making the work of the R user more findable, more accessible, more interoperable and more reusable by other users; Makes the placement into relational databases, semantic web applications, archives, repositories possible without time-consuming and costly data wrangling (See From dataset To RDF). Our package functions work with any structured R objects (data.fame, data.table, tibble, or well-structured lists like json), however, the best functionality is achieved by the (See The dataset S3 Class), which is inherited from data.frame().\nContact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the dataset project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1660176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660208400,"objectID":"26ed66c19df0a261920b3b3afb0f0370","permalink":"https://competition.dataobservatory.eu/software/dataset/","publishdate":"2022-08-11T00:00:00Z","relpermalink":"/software/dataset/","section":"software","summary":"Publish your datasets with FAIR metadata in open science repositories and place them on knowledge graphs","tags":["Reproducible research","SDMX","W3C","semantic web"],"title":"dataset: Create Interoperable FAIR Datasets","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":" Visit the documentation website of statcodelists on statcodelists.dataobservatory.eu/. The goal of statcodelists is to promote the reuse and exchange of statistical information and related metadata with making the internationally standardized SDMX code lists available for the R user. SDMX – the Statistical Data and Metadata eXchange has been published as an ISO International Standard (ISO 17369). The metadata definitions, including the codelists are updated regularly according to the standard. The authoritative version of the code lists made available in this package is https://sdmx.org/?page_id=3215/.\nPurpose Cross-domain concepts in the SDMX framework describe concepts relevant to many, if not all, statistical domains. SDMX recommends using these concepts whenever feasible in SDMX structures and messages to promote the reuse and exchange of statistical information and related metadata between organisations.\nCode lists are predefined sets of terms from which some statistical coded concepts take their values. SDMX cross-domain code lists are used to support cross-domain concepts. What are these cross-domain coded concepts?\nGeographical codes, like NL = the Netherlands in the CL_AREA code list. Standard industry codes J631 for Data processing, hosting and related activities in Europe. (NACE Rev 2 in Europe, beware, it is J592in Australia and New Zealand, see CL_ACTIVITY_ANZSIC06.) Occupations, like OC2521 for Database designers and administrators in CL_OCCUPATIONS Time fomatting standards, like CCYY for annual data series in CL_TIME_FORMAT. Check out the available codlists on the package homepage.\nThe use of common code lists will help users to work even more efficiently, easing the maintenance of and reducing the need for mapping systems and interfaces delivering data and metadata to them. A very obvious advantage of using the code systems is that you can retrieve data from national sources indifferent of the natural language used in North Macedonia, Japan, the U.S. or the Netherlands. While the data labels may change to be locally human-readable, computers and geeks can read the codes and understand them immediately. Provided that they use the standard codes.\nOur data observatories are rolling out SDMX coding across all datasets to help data ingestion and interoperability, data findability and data reuse. statcodelists can help the use of standard SDMX codes in your R workflow–both for downloading data from statistical agencies and to produce publication-ready datasets that the rest of the world (and even APIs) will understand.\nInstallation You can install statcodelists from CRAN:\ninstall.packages(\u0026#34;statcodelists\u0026#34;) Further recommended code values for expressing general statistical concepts like not applicable, etc., can be found in section Generic codes of the Guidelines for the creation and management of SDMX Cross-Domain Code Lists.\nFor further codelists used by reliable statistical agency but not harmonized on SDMX level please consult the SDMX Global Registry Codelists page.\nThe creator of this package is not affiliated with SDMX, and this package was has not been endorsed by SDMX.\nCode of Conduct Please note that the statcodelists project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\n","date":1656486720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656486720,"objectID":"93e74f22e59182e38c653816a3412e7d","permalink":"https://competition.dataobservatory.eu/post/2022-06-29-statcodelists/","publishdate":"2022-06-29T08:12:00+01:00","relpermalink":"/post/2022-06-29-statcodelists/","section":"post","summary":"A new building block of our observatories went through code peer review and was released yesterday. The statcodelists R package aim to promote the  reuse and exchange of statistical information and related metadata with making the internationally standardized SDMX code lists available for the R user.","tags":["R","metadata","statistics","SDMX"],"title":"stacodelists: use standard, language-independent variable codes to help international data interoperability and machine reuse in R","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Retrospective data harmonization The aim of retroharmonize is to provide tools for reproducible retrospective (ex-post) harmonization of datasets that contain variables measuring the same concepts but coded in different ways. Ex-post data harmonization enables better use of existing data and creates new research opportunities. For example, harmonizing data from different countries enables cross-national comparisons, while merging data from different time points makes it possible to track changes over time.\nRetrospective data harmonization is associated with challenges including conceptual issues with establishing equivalence and comparability, practical complications of having to standardize the naming and coding of variables, technical difficulties with merging data stored in different formats, and the need to document a large number of data transformations. The retroharmonize package assists with the latter three components, freeing up the capacity of researchers to focus on the first.\nSpecifically, the retroharmonize package proposes a reproducible workflow, including a new class for storing data together with the harmonized and original metadata, as well as functions for importing data from different formats, harmonizing data and metadata, documenting the harmonization process, and converting between data types. See here for an overview of the functionalities.\nThe new labelled_spss_survey() class is an extension of haven’s labelled_spss class. It not only preserves variable and value labels and the user-defined missing range, but also gives an identifier, for example, the filename or the wave number, to the vector. Additionally, it enables the preservation – as metadata attributes – of the original variable names, labels, and value codes and labels, from the source data, in addition to the harmonized variable names, labels, and value codes and labels. This way, the harmonized data also contain the pre-harmonization record. The stored original metadata can be used for validation and documentation purposes.\nThe vignette Working With The labelled_spss_survey Class provides more information about the labelled_spss_survey() class.\nIn Harmonize Value Labels we discuss the characteristics of the labelled_spss_survey() class and demonstrates the problems that using this class solves.\nWe also provide three extensive case studies illustrating how the retroharmonize package can be used for ex-post harmonization of data from cross-national surveys:\nAfrobarometer Arab Barometer Eurobarometer The creators of retroharmonize are not affiliated with either Afrobarometer, Arab Barometer, Eurobarometer, or the organizations that designs, produces or archives their surveys.\nWe started building an experimental APIs data is running retroharmonize regularly and improving known statistical data sources. See: Digital Music Observatory, Green Deal Data Observatory, Economy Data Observatory.\nCitations and related work Citing the data sources Our package has been tested on three harmonized survey’s microdata. Because retroharmonize is not affiliated with any of these data sources, to replicate our tutorials or work with the data, you have download the data files from these sources, and you have to cite those sources in your work.\nAfrobarometer data: Cite Afrobarometer Arab Barometer data: cite Arab Barometer. Eurobarometer data: The Eurobarometer data Eurobarometer raw data and related documentation (questionnaires, codebooks, etc.) are made available by GESIS, ICPSR and through the Social Science Data Archive networks. You should cite your source, in our examples, we rely on the GESIS data files.\nCiting the retroharmonize R package For main developer and contributors, see the package homepage.\nThis work can be freely used, modified and distributed under the GPL-3 license:\ncitation(\u0026#34;retroharmonize\u0026#34;) #\u0026gt; #\u0026gt; To cite package \u0026#39;retroharmonize\u0026#39; in publications use: #\u0026gt; #\u0026gt; Daniel Antal (2021). retroharmonize: Ex Post Survey Data #\u0026gt; Harmonization. R package version 0.1.17. #\u0026gt; https://retroharmonize.dataobservatory.eu/ #\u0026gt; #\u0026gt; A BibTeX entry for LaTeX users is #\u0026gt; #\u0026gt; @Manual{, #\u0026gt; title = {retroharmonize: Ex Post Survey Data Harmonization}, #\u0026gt; author = {Daniel Antal}, #\u0026gt; year = {2021}, #\u0026gt; doi = {10.5281/zenodo.5006056}, #\u0026gt; note = {R package version 0.1.17}, #\u0026gt; url = {https://retroharmonize.dataobservatory.eu/}, #\u0026gt; } Contact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the retroharmonize project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660201200,"objectID":"cfc0014323d64a4fbf879405539ea261","permalink":"https://competition.dataobservatory.eu/software/statcodelists/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/software/statcodelists/","section":"software","summary":"The goal of retroharmonize is to facilitate retrospective (ex-post) harmonization of data, particularly survey data, in a reproducible manner.","tags":["statcodelists"],"title":"statcodelists: Use Standardized Statistical Codelists","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":"In this example, we show a simple indicator: the Turnover in Radio Broadcasting Enterprises in many European countries. This is an important demand driver in the Music economy pillar of our Digital Music Observatory, and important indicator in our more general Cultural \u0026amp; Creative Sectors and Industries Observatory. Of course, if you work with competition policy or antitrust, than any industry may be interesting to you–but not all of them are well-serverd with data.\nThis dataset comes from a public datasource, the data warehouse of the European statistical agency, Eurostat. Yet it is not trivial to use: unless you are familiar with national accounts, you will not find this dataset on the Eurostat website.\nThe data can be retrieved from the Annual detailed enterprise statistics for services NACE Rev.2 H-N and S95 Eurostat folder. Our version of this statistical indicator is documented following the FAIR principles: our data assets are findable, accessible, interoperable, and reusable. While the Eurostat data warehouse partly fulfills these important data quality expectations, we can improve them significantly. And we can also improve the dataset, too, as we will show in the next blogpost.\nFindable Data Our data observatories add value by curating the data–we bring this indicator to light with a more descriptive name, and we place it in a domain-specific context with our Digital Music Observatory and Cultural \u0026amp; Creative Sectors and Industries Observatory and a policy-specific context with our Competition Data Observatory and Green Deal Data Observatory. While many people may need this dataset in the creative sectors, or among cultural policy designers, most of them have no training in working with national accounts, which imply decyphering national account data codes in records that measure economic activity at a national level. Our curated data observatories bring together many available data around important domains. Our Digital Music Observatory, for example, aims to form an ecosystem of music data users and producers.\nWe added descriptive metadata that help you find our data and match it with other relevant data sources. We added descriptive metadata that help you find our data and match it with other relevant data sources. For example, we add keywords and standardized metadata identifiers from the Library of Congress Linked Data Services, probably the world’s largest standardized knowledge library description. This ensures that you can find relevant data around the same key term (radio broadcasting) in addition to our turnover data. This allows connecting our dataset unambiguosly with other information sources that use the same concept, but may be listed under different keywords, such as Radio–Broadcasting, or Radio industry and trade, or maybe Hörfunkveranstalter in German, or Emitiranje radijskog programa in Croatian or Actividades de radiodifusão in Portugese.\nAccessible Data Our data is accessible in two forms: in csv tabular format (which can be read with Excel, OpenOffice, Numbers, SPSS and many similar spreadsheet or statistical applications) and in JSON for automated importing into your databases. We can also provide our users with SQLite databases, which are fully functional, single user relational databases.\nTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This makes the data easier to clean, and far more easier to use in a much wider range of applications than the original data we used. In theory, this is a simple objective, yet we find that even governmental statistical agencies–and even scientific publications–often publish untidy data. This poses a significant problem that implies productivity loses: tidying data will require long hours of investment, and if a reproducible workflow is not used, data integrity can also be compromised: chances are that the process of tidying will overwrite, delete, or omit a data or a label.\nTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. While the original data source, the Eurostat data warehouse is accessible, too, we added value with bringing the data into a tidy format. Tidy data can immediately be imported into a statistical application like SPSS or STATA, or into your own database. It is immediately available for plotting in Excel, OpenOffice or Numbers.\nInteroperability Our data can be easily imported with, or joined with data from other internal or external sources.\nAll our indicators come with standardized descriptive metadata, and statistical (processing) metadata. See our API All our indicators come with standardized descriptive metadata, following two important standards, the Dublin Core and DataCite–implementing not only the mandatory, but the recommended …","date":1636362000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636362000,"objectID":"0f375332304d9b26d06a73003cca93bd","permalink":"https://competition.dataobservatory.eu/post/2021-11-08-indicator_findable/","publishdate":"2021-11-08T09:00:00Z","relpermalink":"/post/2021-11-08-indicator_findable/","section":"post","summary":"Many people ask if we can really add value to free data that can be downloaded from the Internet by anybody. We do not only work with easy-to-download data, but we know that free, public data usually requires a lot of work to become really valuable. To start with, it is not always easy to find.","tags":["radio broadcasting","data-as-service","API","metadata","FAIR principle","data interoperability","better documentation","data curation"],"title":"How We Add Value to Public Data With Better Curation And Documentation?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Public data sources are often plagued by missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information. This approach of ignoring or dropping missing values will not be feasible or robust when you want to make a beautiful visualization, or use data in a business forecasting model, a machine learning (AI) applicaton, or a more complex scientific model. All of the above require complete datasets, and naively discarding missing data points amounts to an excessive waste of information. In this example we are continuing the example a not-so-easy to find public dataset.\nIn the previous blogpost we explained how we added value by documenting data following the FAIR principle and with the professional curatorial work of placing the data in context, and linking it to other information sources, such as other datasets, books, and publications, regardless of their natural language (i.e., whether these sources are described in English, German, Portugese or Croatian). Photo: Jack Sloop. Completing missing datapoints requires statistical production information (why might the data be missing?) and data science knowhow (how to impute the missing value.) If you do not have a good statistician or data scientist in your team, you will need high-quality, complete datasets. This is what our automated data observatories provide.\nWhy is data missing? International organizations offer many statistical products, but usually they are on an ‘as-is’ basis. For example, Eurostat is the world’s premiere statistical agency, but it has no right to overrule whatever data the member states of the European Union, and some other cooperating European countries give to them. And they cannot force these countries to hand over data if they fail to do so. As a result, there will be many data points that are missing, and often data points that have wrong (obsolete) descriptions or geographical dimensions. We will show the geographical aspect of the problem in a separate blogpost; for now, we only focus on missing data.\nSome countries have only recently started providing data to the Eurostat umbrella organization, and it is likely that you will find few datapoints for North Macedonia or Bosnia-Herzegovina. Other countries provide data with some delay, and the last one or two years are missing. And there are gaps in some countries’ data, too.\nSee the authoritative copy of the dataset. This is a headache if you want to use the data in some machine learning application or in a multiple or panel regression model. You can, of course, discard countries or years where you do not have full data coverage, but this approach usually wastes too much information–if you work with 12 years, and only one data point is available, you would be discarding an entire country’s 11-years’ worth of data. Another option is to estimate the values, or otherwise impute the missing data, when this is possible with reasonable precision. This is where things get tricky, and you will likely need a statistician or a data scientist onboard.\nWhat can we improve? Consider that the data is only missing from one year for a particular country, 2015. The naive solution would be to omit 2015 or the country at hand from the dataset. This is pretty destructive, because we know a lot about the radio market turnover in this country and in this year! But leaving 2015 blank will not look good on a chart, and will make your machine learning application or your regression model stop.\nA statistician or a radio market expert will tell you that you know more-or-less the missing information: the total turnover was certainly not zero in that year. With some statistical or radio domain-specific knowledge you will use the 2014, or 2016 value, or a combination of the two and keep the country and year in the dataset.\nOur improved dataset added backcasted (using the best time series model fitting the country’s actually present data), forecasted (again, using the best time series model), and approximated data (using linear approximation.) In a few cases, we add the last or next known value. To give a few quantiative indicators about our work:\nIncreased number of observations: 65% Reduced missing values: -48.1% Increased non-missing subset for regression or AI: +66.67% If your organization is working with panel (longitudional multiple) regressions or various machine learning applications, then your team knows that not havint the +66.67% gain would be a deal-breaker in the choice of models and punctuality of estimates or KPIs or other quantiative products. And that they would spent about 90% of their data resources on achieving this +66.67% gain in usability.\nIf you happen to work in an NGO, a business unit or a research institute that does not employ data scientists, then it is likely that you can never achieve this improvement, and you have to give up on a number of quantitative tools or …","date":1636362000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636624860,"objectID":"64c18e37a183412e97b5041bcadff5d2","permalink":"https://competition.dataobservatory.eu/post/2021-11-06-indicator_value_added/","publishdate":"2021-11-08T10:00:00+01:00","relpermalink":"/post/2021-11-06-indicator_value_added/","section":"post","summary":"Public data sources are often plagued with missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information which will destroy your beautiful visualization or stop your application from working. In this example we show how we increase the usable subset of a public dataset by 66.7%, rendering useful what would otherwise have been a deal-breaker in panel regressions or machine learning applications.","tags":["Digital Music Observatory","data-as-service","API","metadata","forecasting","missing data"],"title":"How We Add Value to Public Data With Imputation and Forecasting","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":" Sisyphus was punished by being forced to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity. This is the price that project managers and analysts pay for the inadequate documentation of their data assets. When was a file downloaded from the internet? What happened with it sense? Are their updates? Did the bibliographical reference was made for quotations? Missing values imputed? Currency translated? Who knows about it – who created a dataset, who contributed to it? Which is an intermediate format of a spreadsheet file, and which is the final, checked, approved by a senior manager?\nBig data creates inequality and injustice. On aspect of this inequality is the cost of data processing and documentation – a greatly underestimated, and usually not reported cost item. In small organizations, where there are no separate data science and data engineering roles, data is usually supposed to be processed and documented by (junior) analysts or researchers. This a very important source of the gap between Big Tech and them: the data usually ends up very expensive, ill-formatted, not readable by computers that use machine learning and AI. Usually the documentation steps are completely omitted.\n“Data is potential information, analogous to potential energy: work is required to release it.” – Jeffrey Pomerantz\nMetadata, which is information about the history of the data, and information how it can be technically and legally reused, has a hidden cost. Cheap or low-quality external data comes with poor or no metadata, and small organizations lack the resources to add high-quality metadata to their datasets. However, this only perpetuates the problem.\nThe hidden cost item behind the unbillable hours As we have shown with our research partners, such metadata problems are not unique to data analysis. Independent artists and small labels are suffering on music or book sales platforms, because their copyrighted content is not well documented. If you automatically document tens of thousands of songs or datasets, the documentation cost is very small per item. If you, do it manually, the cost may be higher than the expected revenue from the song, or the total cost of the dataset itself. (See our research consortiums’ preprint paper: Ensuring the Visibility and Accessibility of European Creative Content on the World Market: The Need for Copyright Data Improvement in the Light of New Technologies)\nIn the short run, small consultancies, NGOs, or as a matter of fact, musicians, seem to logically give up on high-quality documentation and logging. In the long run, this has two devastating consequences: computers, such as machine learning algorithms cannot read their documents, data, songs. And as memory fades, the ill-documented resources need to be re-created, re-checked, reformatted. Often, they are even hard to find on your internal server or laptop archive.\nMetadata is a hidden destroyer of the competitiveness of corporate or academic research, or independent content management. It never quoted on external data vendor invoices, it is not planned as a cost item, because metadata, the description of a dataset, a document, a presentation, or song, is meaningless without the resource that it describes. You never buy metadata. But if your dataset comes without proper metadata documentation, you are bound, like Sisyphus, to search for it, to re-arrange it, to check its currency units, its digits, its formatting. Data analysts are reported to spend about 80% of their working hours on data processing and not data analysis – partly, because data processing is a very laborious task that can be done by computers at a scale far cheaper, and partly because they do not know if the person who sat before them at the same desk has already performed these tasks, or if the person responsible for quality control checked for errors.\nUncut diamonds need to be cut, polished, and you have to make sure that they come from a legal source. Data is similar: it needs to be tidied up, checked and documented before use. Photo: Dave Fischer. Undocumented data is hardly informative – it may be a page in a book, a file in an obsolete file format on a governmental server, an Excel sheet that you do not remember to have checked for updates. Most data are useless, because we do not know how it can inform us, or we do not know if we can trust it. The processing can be a daunting task, not to mention the most boring and often neglected documentation duties after the dataset is final and pronounced error-free by the person in charge of quality control.\nOur observatory automatically processes and documents the data The good news about documentation and data validation costs is that they can be shared. If many users need GDP/capita data from all over the world in euros, then it is enough if only one entity, a data observatory, collects all GDP and population data expresed in dollars, korunas, and euros, …","date":1625734800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625734800,"objectID":"7fe21d2727803b3edcddd706a67fe6a2","permalink":"https://competition.dataobservatory.eu/post/2021-07-08-data-sisyphus/","publishdate":"2021-07-08T09:00:00Z","relpermalink":"/post/2021-07-08-data-sisyphus/","section":"post","summary":"Sisyphus was punished by being forced to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity.  When was a file downloaded from the internet?  What happened with it sense?  Are their updates? Did the bibliographical reference was made for quotations?  Missing values imputed?  Currency translated? Who knows about it – who created a dataset, who contributed to it?  Which is the final, checked, approved by a senior manager?","tags":["metadata","data-as-service","api"],"title":"The Data Sisyphus","type":"post"},{"authors":null,"categories":null,"content":"Adding metadata exponentially increases the value of data. Did your region add a new town to its boundaries? How do you adjust old data to conform to constantly changing geographic boundaries? What are some practical ways of combining satellite sensory data with my organization’s records? And do I have the right to do so? Metadata logs the history of data, providing instructions on how to reuse it, also setting the terms of use. We automate this labor-intensive process applying the FAIR data concept.\nIn our observatory we apply the concept of FAIR (findable, accessibe, interoperable, and reusable digital assets) in our APIs and in our open-source statistical software packages.\nThe hidden cost item Metadata gets less attention than data, because it is never acquired separately, it is not on the invoice, and therefore it remains an a hidden cost, and it is more important from a budgeting and a usability point of view than the data itself. Metadata is responsible for industry non-billable hours or uncredited working hours in academia. Poor data documentation, lack of reproducible processing and testing logs, inconsistent use of currencies, keywords, and storing messy data make reusability and interoperability, integration with other information impossible.\nFAIR Data and the Added Value of Rich Metadata we introduce how we apply the concept of FAIR (findable, accessibe, interoperable, and reusable digital assets) in our APIs.\nOrganizations pay many times for the same, repeated work, because these boring tasks, which often comprise of tens of thousands of microtasks, are neglected. Our solution creates automatic documentation and metadata for your own historical internal data or for acquisitions from data vendors. We apply the more general Dublin Core and the more specific, mandatory and recommended values of DataCite for datasets – these are new requirements in EU-funded research from 2021. But they are just the minimal steps, and there is a lot more to do to create a diamond ring from an uncut gem.\nMap your data: bibliographis, catalogues, codebooks, versioning Updating descriptive metadata, such as bibliographic citation files, descriptions and sources to data files downloaded from the internet, versioning spreadsheet documents and presentations is usually a hated and often neglected task withing organization, and rightly so: these boring and error-prone tasks are best left to computers.\nAlready adjusted spreadsheets are re-adjusted and re-checked. Hours are spent on looking for the right document with the rigth version. Duplicates multiply. Already downloaded data is downloaded again, and miscategorized, again. Finding the data without map is a treasure hunt. Photo: © N. The lack of time and resources spend on documentation over time reduces reusability and significantly increases data processing and supervision or auditing costs.\nOur observatory metadata is compliant with the Dublin Core Cross-Domain Attribute Set metadata standard, but we use different formatting. We offer simple re-formatting from the richer DataCite to Dublin Core for interoperability with a wider set of data sources. We use all mandatory DataCite metadata fields, all the the recommended and optional ones. It complies with the tidy data principles. In other words: very easy to import into your databases, or join with other databases, and the information is easy to find. Corrections, updates can automatically managed.\nWhat happened with the data before? We are creating Codebooks that are following the SDMX statistical metadata codelists, and resemble the SMDX concepts used by international statistical agencies. (See more technical information here.) Small organizations often cannot afford to have data engineers and data scientists on staff, and they employ analysts who work with Excel, OpenOffice, PowerBI, SPSS or Stata. The problem with these applications is that they often require the user to manually adjust the data, with keyboard entries or mouse clicks. Furthermore, they do not provide a precise logging of the data processing, manipulation history. The manual data processing and manipulation is very error prone and makes the use of complex and high value resources, such as harmonized surveys or symmetric input-output tables, to name two important source we deal with, impossible to use. The use of these high-value data sources often requires tens of thousands of data processing steps: no human can do it faultlessly.\nWhat is even more problematic that simple applications for analysis do not provide a log of these manipulations’ steps: pulling over a column with the mouse, renaming a row, adding a zero to an empty cell. This makes senior supervisory oversight and external audit very costly.\nOur data comes with full history: all changes are visible, and we even open the code or algorithm that processed the raw data. Your analysts can still use their favourite spreadsheet or statistical software application, but they can start from a …","date":1625616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"8ac5f27972f957d4b5d4188584738e4c","permalink":"https://competition.dataobservatory.eu/services/metadata/","publishdate":"2021-07-07T00:00:00Z","relpermalink":"/services/metadata/","section":"services","summary":"Adding metadata exponentially increases the value of data. Did somebody already adjust old data to conform to constantly changing geographic boundaries? What are some practical ways of combining satellite sensory data with my organization's records? And do I have the right to do so? Metadata logs the history of data, providing instructions on how to reuse it, also setting the terms of use. We automate this labor-intensive process applying the FAIR data concept.","tags":["metadata"],"title":"Metadata","type":"services"},{"authors":null,"categories":null,"content":"We provide retrospecitve, ex post, and ex ante survey harmonization to our partners.\nThe aim of retrospective survey harmonization is to pool data from pre-existing surveys made with a similar methodology in different points in time and different countries or territories. Ex post survey harmonization is in a way a passive form of pooling research funding because you can utilize information from surveying that were made on somebody else’s expense. The Arab Barometer surveys do not have a consolidated codebook, but our retroharmonize software created one, and put together data from three years and collected in many countries about various public policy issues. The aim of ex ante survey harmonization is to maximize the value from future retrospective harmonization; in a way, it is an active form of pooling research funding, because you benefit from money spent on related open governmental and open science survey programs. In this example we designed a survey representative among music professionals that it can be compared with large-sample, national surveys on living conditions and attitudes, and with occupational groups. Nationally representative surveys do not question enough musicians to allow such specific use; musician only surveys do not allow comparison. retorhamonize is a peer-reviewed, scientfic statistcal software that allows the programmatic retrospective harmonization of surveys, such as the last 35 years of all Eurobarometer microdata, or all Afrobarometer microdata. Eurobarometer grew out of certain CEE member states’ need for comparable data about their music and audiovisual sectors. We commissioned surveys following ESSNet-Culture guidelines and combined our survey data with open access European microdata-level surveys.\nregions solves the problems caused by Europe’s shifting regional boundaries, which have undergone changes in several thousand places over the last twenty years, meaning member states’ and Eurostat’s regional statistics are not comparable over more than two to three years. This software validates and, where possible, changes the regional coding from NUTS1999 until the not yet used NUTS2021, opening up vast, valuable, untapped data sources that can be used for longitudinal analysis or for panel analysis far more precise than what national data alone would allow. It was originally designed in a research project at IVIR in the University of Amsterdam to understand the geographical dynamics of book piracy. Because of the needs this software fills, it had 700 users in the first month after publication. It is particularly useful to re-code old surveys, as regional boundaries are changing in each decade several hundred times in Europe.\n","date":1625472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625472000,"objectID":"395517f5bd9a85f85c9fbd68e92b6d7d","permalink":"https://competition.dataobservatory.eu/data/surveys/","publishdate":"2021-07-05T08:00:00Z","relpermalink":"/data/surveys/","section":"data","summary":"Ex post survey harmonization is in a way a passive form of pooling research funding because you can utilize information from surveying that were made on somebody else’s expense.  The aim of ex ante survey harmonization is to maximize the value from future retrospective harmonization; in a way, it is an active form of pooling research funding, because you benefit from money spent on related open governmental and open science survey programs.","tags":["surveys","survey harmonization"],"title":"Survey Harmonization","type":"data"},{"authors":["Daniel Antal"],"categories":null,"content":"A new version of the retroharmonize R package – which is working with retrospective, ex post harmonization of survey data – was released yesterday after peer-review on CRAN. It allows us to compare opinion polling data from the Arab Barometer with the Eurobarometer and Afrorbarometer. This is the first version that is released in the rOpenGov community, a community of R package developers on open government data analytics and related topics.\nSurveys are the most important data sources in social and economic statistics – they ask people about their lives, their attitudes and self-reported actions, or record data from companies and NGOs. Survey harmonization makes survey data comparable across time and countries. It is very important, because often we do not know without comparison if an indicator value is low or high. If 40% of the people think that climate change is a very serious problem, it does not really tell us much without knowing what percentage of the people answered this question similarly a year ago, or in other parts of the world.\nWith the help of Ahmed Shabani and Yousef Ibrahim, we created a third case study after the Eurobarometer, and Afrobarometer, about working with the Arab Barometer harmonized survey data files.\nEx ante survey harmonization means that researchers design questionnaires that are asking the same questions with the same survey methodology in repeated, distinct times (waves), or across different countries with carefully harmonized question translations. Ex post harmonizations means that the resulting data has the same variable names, same variable coding, and can be joined into a tidy data frame for joint statistical analysis. While seemingly a simple task, it involves plenty of metadata adjustments, because established survey programs like Eurobarometer, Afrobarometer or Arab Barometer have several decades of history, and several decades of coding practices and file formatting legacy.\nVariable harmonization means that if the same question is called in one microdata source Q108 and the other eval-parl-elections then we make sure that they get a harmonize and machine readable name without spaces and special characters. Variable label harmonization means that the same questionnaire items get the same numeric coding and same categorical labels. Missing case harmonization means that various forms of missingness are treated the same way. For the evaluation of the economic situation dataset, get the country averages and aggregates from Zenodo, and the plot in jpg or png from figshare. In our new Arab Barometer case study, the evaulation of parliamentary elections has the following labels. We code them consistently 1 = free_and_fair, 2 = some_minor_problems, 3 = some_major_problems and 4 = not_free.\n“0. missing” “1. they were completely free and fair” “2. they were free and fair, with some minor problems” “3. they were free and fair, with some major problems” “4. they were not free and fair” “8. i don’t know” “9. declined to answer” “Missing” “They were completely free and fair” “They were free and fair, with some minor breaches” “They were free and fair, with some major breaches” “They were not free and fair” “Don’t know” “Refuse” “Completely free and fair” “Free and fair, but with minor problems” “Free and fair, with major problems” “Not free or fair” “Don’t know (Do not read)” “Decline to answer (Do not read)” Of course, this harmonization is essential to get clean results like this:\nFor evaluation or reuse of parliamentary elections dataset get the replication data and the code from the Zenodo open repository. In our case study, we had three forms of missingness: the respondent did not know the answer, the respondent did not want to answer, and at last, in some cases the respondent was not asked, because the country held no parliamentary elections. While in numerical processing, all these answers must be left out from calculating averages, for example, in a more detailed, categorical analysis they represent very different cases. A high level of refusal to answer may be an indicator of surpressing democratic opinion forming in itself.\nSurvey harmonization with many countries entails tens of thousands of small data management task, which, unless automatically documented, logged, and created with a reproducible code, is a helplessly error-prone process. We believe that our open-source software will bring many new statistical information to the light, which, while legally open, was never processed due to the large investment needed.\nWe also started building experimental APIs data is running retroharmonize regularly. We will place cultural access and participation data in the Digital Music Observatory, climate awareness, policy support and self-reported mitigation strategies into the Green Deal Data Observatory, and economy and well-being data into our Economy Data Observatory.\nFurther plans Retrospective survey harmonization is a far more complex task than this blogpost suggest. …","date":1624870800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624870800,"objectID":"ed5892c8c036e56b13a3ea3390d80d8d","permalink":"https://competition.dataobservatory.eu/post/2021-06-28-arabbarometer/","publishdate":"2021-06-28T09:00:00Z","relpermalink":"/post/2021-06-28-arabbarometer/","section":"post","summary":"A new version of the retroharmonize R package – which is working with retrospective, ex post harmonization of survey data – was released yesterday after peer-review on CRAN. It allows us to compare opinion polling data from the Arab Barometer with the Eurobarometer and Afrorbarometer. This is the first version that is released in the rOpenGov community, a community of R package developers on open government data analytics and related topics.","tags":["open-data","open-science","R","data collection","Arab Barometer","survey harmonization"],"title":"Including Indicators from Arab Barometer in Our Observatory","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"If open data is the new gold, why even those who release fail to reuse it? We created an open collaboration of data curators and open-source developers to dig into novel open data sources and/or increase the usability of existing ones. We transform reproducible research software into research- as-service.\nEvery year, the EU announces that billions and billions of data are now “open” again, but this is not gold. At least not in the form of nicely minted gold coins, but in gold dust and nuggets found in the muddy banks of chilly rivers. There is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions.\nThere is no rush for it, because panning out its value requires a lot of hours of hard work. Our goal is to automate this work to make open data usable at scale, even in trustworthy AI solutions. Most open data is not public, it is not downloadable from the Internet – in the EU parlance, “open” only means a legal entitlement to get access to it. And even in the rare cases when data is open and public, often it is mired by data quality issues. We are working on the prototypes of a data-as-service and research-as-service built with open-source statistical software that taps into various and often neglected open data sources.\nWe are in the prototype phase in June and our intentions are to have a well-functioning service by the time of the conference, because we are working only with open-source software elements; our technological readiness level is already very high. The novelty of our process is that we are trying to further develop and integrate a few open-source technology items into technologically and financially sustainable data-as-service and even research-as-service solutions.\nOur review of about 80 EU, UN and OECD data observatories reveals that most of them do not use these organizations’s open data - instead they use various, and often not well processed proprietary sources. We are taking a new and modern approach to the data observatory concept, and modernizing it with the application of 21st century data and metadata standards, the new results of reproducible research and data science. Various UN and OECD bodies, and particularly the European Union support or maintain more than 60 data observatories, or permanent data collection and dissemination points, but even these do not use these organizations and their members open data. We are building open-source data observatories, which run open-source statistical software that automatically processes and documents reusable public sector data (from public transport, meteorology, tax offices, taxpayer funded satellite systems, etc.) and reusable scientific data (from EU taxpayer funded research) into new, high quality statistical indicators.\nWe are taking a new and modern approach to the ‘data observatory’ concept, and modernizing it with the application of 21st century data and metadata standards, the new results of reproducible research and data science We are building various open-source data collection tools in R and Python to bring up data from big data APIs and legally open, but not public, and not well served data sources. For example, we are working on capturing representative data from the Spotify API or creating harmonized datasets from the Eurobarometer and Afrobarometer survey programs. Open data is usually not public; whatever is legally accessible is usually not ready to use for commercial or scientific purposes. In Europe, almost all taxpayer funded data is legally open for reuse, but it is usually stored in heterogeneous formats, processed into an original government or scientific need, and with various and low documentation standards. Our expert data curators are looking for new data sources that should be (re-) processed and re-documented to be usable for a wider community. We would like to introduce our service flow, which touches upon many important aspects of data scientist, data engineer and data curatorial work. We believe that even such generally trusted data sources as Eurostat often need to be reprocessed, because various legal and political constraints do not allow the common European statistical services to provide optimal quality data – for example, on the regional and city levels. With rOpenGov and other partners, we are creating open-source statistical software in R to re-process these heterogenous and low-quality data into tidy statistical indicators to automatically validate and document it. We are carefully documenting and releasing administrative, processing, and descriptive metadata, following international metadata standards, to make our data easy to find and easy to use for data analysts. We are automatically creating depositions and authoritative copies marked with an individual digital object identifier (DOI) to maintain data integrity. We are building simple databases and supporting APIs …","date":1624035600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624035600,"objectID":"a5962e972994ae1aa5811355d21781c6","permalink":"https://competition.dataobservatory.eu/post/2021-06-18-gold-without-rush/","publishdate":"2021-06-18T17:00:00Z","relpermalink":"/post/2021-06-18-gold-without-rush/","section":"post","summary":"If open data is the new gold, why even those who release fail to reuse it? We created an open collaboration of data curators and open-source developers to dig into novel open data sources and/or increase the usability of existing ones. We transform reproducible research software into research- as-service.","tags":["open-data","open-science","R","data collection"],"title":"Open Data - The New Gold Without the Rush","type":"post"},{"authors":["Daniel Antal","rOpenGov","Leo Lahti","Kasia Kulma"],"categories":null,"content":" The new version of our rOpenGov R package regions was released today on CRAN. This package is one of the engines of our experimental open data-as-service Green Deal Data Observatory , Economy Data Observatory , Digital Music Observatory prototypes, which aim to place open data packages into open-source applications.\nIn international comparison the use of nationally aggregated indicators often have many disadvantages: they inhibit very different levels of homogeneity, and data is often very limited in number of observations for a cross-sectional analysis. When comparing European countries, a few missing cases can limit the cross-section of countries to around 20 cases which disallows the use of many analytical methods. Working with sub-national statistics has many advantages: the similarity of the aggregation level and high number of observations can allow more precise control of model parameters and errors, and the number of observations grows from 20 to 200-300.\nThe change from national to sub-national level comes with a huge data processing price: internal administrative boundaries, their names, codes codes change very frequently. Yet the change from national to sub-national level comes with a huge data processing price. While national boundaries are relatively stable, with only a handful of changes in each recent decade. The change of national boundaries requires a more-or-less global consensus. But states are free to change their internal administrative boundaries, and they do it with large frequency. This means that the names, identification codes and boundary definitions of sub-national regions change very frequently. Joining data from different sources and different years can be very difficult.\nOur regions R package helps the data processing, validation and imputation of sub-national, regional datasets and their coding. There are numerous advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation, and the regions package aims to help this process.\nYou can review the problem, and the code that created the two map comparisons, in the Maping Regional Data, Maping Metadata Problems vignette article of the package. A more detailed problem description can be found in Working With Regional, Sub-National Statistical Products.\nThis package is an offspring of the eurostat package on rOpenGov. It started as a tool to validate and re-code regional Eurostat statistics, but it aims to be a general solution for all sub-national statistics. It will be developed parallel with other rOpenGov packages.\nGet the Package You can install the development version from GitHub with:\ndevtools::install_github(\u0026#34;rOpenGov/regions\u0026#34;) or the released version from CRAN:\ninstall.packages(\u0026#34;regions\u0026#34;) You can review the complete package documentation on regions.dataobservaotry.eu. If you find any problems with the code, please raise an issue on Github. Pull requests are welcome if you agree with the Contributor Code of Conduct\nIf you use regions in your work, please cite the package as: Daniel Antal, Kasia Kulma, Istvan Zsoldos, \u0026amp; Leo Lahti. (2021, June 16). regions (Version 0.1.7). CRAN. http://doi.org/10.5281/zenodo.4965909\nJoin us Join our open collaboration Economy Data Observatory team as a data curator, developer or business developer. More interested in environmental impact analysis? Try our Green Deal Data Observatory team! Or your interest lies more in data governance, trustworthy AI and other digital market problems? Check out our Digital Music Observatory team!\n","date":1623844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623844800,"objectID":"bddf0a5d0c8dcdea162b11029d3b81b5","permalink":"https://competition.dataobservatory.eu/post/2021-06-16-regions-release/","publishdate":"2021-06-16T12:00:00Z","relpermalink":"/post/2021-06-16-regions-release/","section":"post","summary":"There are numerous advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation, and the regions package aims to help this process.","tags":["open-data","open-science","regional data","sub-national data","R","data collection"],"title":"There are Numerous Advantages of Switching from a National Level of the Analysis to a Sub National Level","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":" Open data is like gold in the mud below the chilly waves of mountain rivers. Panning it out requires a lot of patience, or a good machine. As the founder of the automated data observatories that are part of Reprex’s core activities, what type of data do you usually use in your day-to-day work?\nThe automated data observatories are results of syndicated research, data pooling, and other creative solutions to the problem of missing or hard-to-find data. The music industry is a very fragmented industry, where market research budgets and data are scattered in tens of thousands of small organizations in Europe. Working for the music and film industry as a data analyst and economist was always a pain because most of the efforts went into trying to find any data that can be analyzed. I spent most of the last 7-8 years trying to find any sort of information—from satellites to government archives—that could be formed into actionable data. I see three big sources of information: textual,numeric, and continuous recordings for on-site, offsite, and satellite sensors. I am much better with numbers than with natural language processing, and I am improving with sensory sources. But technically, I can mint any systematic information—the text of an old book, a satellite image, or an opinion poll—into datasets.\nFor you, what would be the ultimate dataset, or datasets that you would like to see in the Economy Data Observatory?\nI am a data scientist now, but I used to be a regulatory economist, and I have worked a lot with competition policy and monopoly regulation issues. Our observatories can automatically monitor market and environmental processes, which would allow us to get into computational antitrust. Peter Ormosi, our competition curator, is particularly interested in killer acquisitions: approved mergers of big companies that end up piling up patents that are not used. I am more interested in describing systematically which markets are getting more concentrated and more competitive, in real time. Does data concentration coincide with market concentration?\nTo bring an example from the realm of our Digital Music Observatory, which was a prototype to this one, I have been working for some time on creating streaming volume and price indexes, like the Dow Jones Industrial Average or the various bond market indexes, that talk more about price, demand, and potential revenue in music streaming markets all over the world. We did a first take on this in the Central European Music Industry Report and recently we iterated on the model for the UK Intellectual Property Office and the UK Music Creators’ Earnings project. We want to take this further to create a pan-Europe streaming market index, and we will be probably the first to actually be able to report on music market concentrations, and in fact, more or less in a real-time mode.\nWe would like to further developer our 20-country streaming indexes into a global music market index. Is there a number or piece of information that recently surprised you? If so, what was it?\nThere were a few numbers that surprised me, and some of them were brought up by our observatory teams. Karel is talking about the fact that not all green energy is green at all: many hydropower stations contribute to the greenhouse effect and not reduce it. Annette brought up the growing interest in the Dalmatian breed after the Disney 101 Dalmatians movies, and it reminded me of the astonishing growth in interest for chess sets, chess tutorials, and platform subscriptions after the success of Netflix’s The Queen’s Gambit.\nThe Queen’s Gambit’ Chess Boom Moves Online By Rachael Dottle on bloomberg.com Annette is talking about the importance of cultural influencers, and on that theme, what could be more exciting that Netflix’s biggest success so far is not a detective series or a soap opera but a coming-of-age story of a female chess prodigy. Intelligence is sexy, and we are in the intelligence business.\nBut to tell a more serious and more sobering number, I recently read with surprise that there are more people smoking cigarettes on Earth in 2021 than in 1990. Population growth in developing countries replaced the shrinking number of developed country smokers. While I live in Europe, where smoking is strongly declining, it reminds me that Europe’s population is a small part of the world. We cannot take for granted that our home-grown experiences about the world are globally valid.\nDo you have a good example of really good, or really bad use of data?\nFiveThirtyEight.com had a wonderful podcast series, produced by Jody Avirgan, called What’s the Point. It is exactly about good and bad uses of data, and each episode is super interesting. Maybe the most memorable is Why the Bronx Really Burned. New York City tried to measure fire response times, identify redundancies in service, and close or re-allocate fire stations accordingly. What resulted, though, was a perfect storm of bad data: The methodology was flawed, the …","date":1623308400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623315600,"objectID":"3e8ae6bc2572e2e950d012bbe16a1c35","permalink":"https://competition.dataobservatory.eu/post/2021-06-10-founder-daniel-antal/","publishdate":"2021-06-10T07:00:00Z","relpermalink":"/post/2021-06-10-founder-daniel-antal/","section":"post","summary":"Open data is like gold in the mud below the chilly waves of mountain rivers. Panning it out requires a lot of patience, or a good machine. I think we will come to as surprising and strong findings as Bellingcat, but we are not focusing on individual events and stories, but on social and environmental processes and changes.","tags":["open-data","open-science","trustworthy AI","service-design","data collection"],"title":"Open Data is Like Gold in the Mud Below the Chilly Waves of Mountain Rivers","type":"post"},{"authors":["Annette Wong"],"categories":null,"content":"Annette Wong is helping our service development from a digital strategy and marketing point of view.\nWhy is data important to the work that you do as a digital strategist at an agency?\nAs a marketing and digital agency, we work with clients to produce and develop marketing campaigns that impact the bottom line. One of the ways to determine the Return-On-Investment (ROI) is through data. By analyzing the data, our team is able to help our clients predict audience behavior and ideally convert them into taking action ($$$).\nCurrently, I’m working on a music livestreaming platform and everyday we’re always looking at how our campaigns are performing (and measuring their effectiveness). For example, if we’re running a paid campaign through Facebook and if it’s not converting at the expected % that we want, it indicates to us that we need to change our approach. Data gives us the power and freedom to experiment (with minimal risk) and empowers us to make informed decisions quickly.\nWhy are you excited about the Digital Music Observatory and is there a reason you decided to participate in this initiative?\nSeeing how the pandemic decimated the music industry, specifically in-person events, made me feel a lot of empathy for musicians and the economics of their situation, especially with how musicians generate a living income through their music. The importance of data and having open access promotes transparency, fairer wages (ideally), and levels the playing field for musicians of all sizes and popularity.\nOur retroharmonization software helps the creation of objective and comparable indicators about how musicians make a living, or how people think about climate challenges. I decided to participate in this challenge because I love how data is a secret weapon that anyone can use to re-balance the interests of creators, distributors, and consumers.\nIs there a number that recently surprised you? What was it?\nThis is a little silly but very recently I watched the 101 Dalmatians movie. After watching the movie, I was curious to see if there was a correlation between the release of the movie and the number of Dalmations adopted afterwards. 101 Dalmatians was released in 1985 and 1991 which made thousands of families (in the U.S.) want to adopt one. The American Kennel Club reported that the annual number of Dalmatian puppies registered skyrocketed from 8,170 animals to 42,816.\nPhoto: John O’ Groats, Unsplash license. This information is interesting because it validates the idea of how culture influences consumer behavior. I think it’s really cool that we can measure cultural collisions and how it impacts the way we act, think, and respond.\nWhat can our automated data observatories do to make open data more credible in the European economic policy community, or in the music business community more accepted?\nI believe that people, in general, appreciate and understand the importance of data. But, it can be overwhelming, sometimes scary, and intimidating to deal with (esp. in large quantities).\nHowever, I feel more people are open to the idea of using data and understand the value of leveraging data to share objective truths. Something that our automated data observatories can do is to provide more opportunities to educate and train data admirers that data is not scary, that it is accessible, and it is here to help uncover insights that can’t be immediately seen.\nJoin our open collaboration Economy Data Observatory team as a data curator, developer or business developer, or share your data in our public repository Economy Data Observatory on Zenodo Join us Join our open collaboration Economy Data Observatory team as a data curator, developer or business developer. More interested in environmental impact analysis? Try our Green Deal Data Observatory team! Or your interest lies more in data governance, trustworthy AI and other digital market problems? Check out our Digital Music Observatory team!\n","date":1623240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623240000,"objectID":"11669a28cf921bc5c15403604f92562b","permalink":"https://competition.dataobservatory.eu/post/2021-06-09-team-annette-wong/","publishdate":"2021-06-09T12:00:00Z","relpermalink":"/post/2021-06-09-team-annette-wong/","section":"post","summary":"101 Dalmatians was released in 1985 and 1991 which made thousands of families (in the U.S.) want to adopt one. The American Kennel Club reported that the annual number of Dalmatian puppies registered skyrocketed from 8,170 animals to 42,816.","tags":["open-data","open-science","trustworthy AI","service-design","communication"],"title":"Educate and Train Data Admirers that Data is not Scary","type":"post"},{"authors":["Karel Volkaert"],"categories":null,"content":"As a consultant, what type of data do you usually work with?\nI work at the intersection between strategy, finance and organisation. My usual dataset is quite broad - and sometimes unstructured. Oftentimes, the most decisive data are ones that cross domains: economic data coupled with environmental measurements, sociodemographic characteristics linked with online analytics.\nIf you were able to pick, what would be the ultimate dataset, or datasets that you would like to see in the Green Deal Data Observatory? And the Economy Data Observatory?\nIf I may venture that far, the interesting point is where these two data observatories meet. But high on my wishlist would be anything related to geospatial dispersion of environmental and climate data: land erosion, aerosols, solar incidence. From an economic perspective, my interest would go especially to - again - dispersion across regions or other geographical domains of, say, number of new enterprises, disposable income, tax incidence…\nSee our case study on connecting local tax revenues, climate awareness poll data and drought data in Belgium. Why did you decide to join the challenge and why do you think that this would be a game changer for policymakers and for business leaders?\nThere is, both from an ecological and a societal point of view, an urgent need for open-access, real-time, trustworthy data to base decisions on. Ever since Kydland \u0026amp; Prescott’s analyses of “rules rather than discretion” and even earlier analyses of investment under uncertainty, the dynamic rules for optimal decision-making (including investment) require fast-response reliable data.\nDo you have a favorite, or most used open governmental or open science data source? What do you think about it? Could it be improved?\nLet me give one example: the AMECO annual macro-economic database is great for long-term historical analyses but its components ought to be real-time available. As an anecdote, as a fund manager in emerging markets we needed to anticipate macro-economic evolutions and in particular the manner in which capital markets anticipate these evolutions by adjusting foreign exchange rates or positioning themselves along yield curves. To some extent, we needed to predict what AMECO would tell us one year later by means of any real-time trustworthy assessments of the financial or economic situation. The latter data is what we would ideally have in an observatory.\nTo some extent, we needed to predict what AMECO would tell us one year later by means of any real-time trustworthy assessments of the financial or economic situation. The latter data is what we would ideally have in an observatory. Is there a piece of information that recently surprised you? What was it?\nI am currently working on water-related issues and came across a result reported in Nature Energy earlier this year that in more than one in ten hydropower stations, the extra warming from the dark surface of the water reservoir was enough to outbalance its “green” electricity generation potential, leading to no net climate benefits.\nThe researchers found that almost half of the reservoirs they surveyed took just four years to reach a net climate benefit. Unfortunately, they also found that 19% of those surveyed took more than 40 years to do so, and approximately 12% of them took 80 years—the average lifetime of a hydroelectric plant. Calculating the albedo-climate penalty of hydropower dammed reservoirs\nAgain: spatial distribution matters…\nPhoto: Kees Streefkerk, Unplash License From your experience, what do you think the greatest problem with open data in 2021 will be?\nTrust. In a society where “value” and even “truth” is determined more by the amount of (web) links to a particular “fact” than by its intrinsic characteristics, we need to be able to trust data — open data because it’s open and “closed” data because it’s closed.\nWhat can our automated data observatories do to make open data more credible in the European economic policy and climate change or mitigation community and be more accepted as verified information?\nIf I may refer to the previous answer: credibility is enhanced through cross-links between different data from different domains that “does not disprove” one another or that is internally consistent. If, say, data on taxable income goes in one direction and taxes in another, it is the reasoned reconciliation of the - alleged or real - inconsistency that will validate the comprehensive data set. So I am a great believer in broad, real-time observatories where not only the data capture, but the data reconciliation is automated, sometimes by means of a simple comparative statics analysis, in other cases maybe through quite elaborate artificial intelligence.\nJoin our open collaboration Economy Data Observatory team as a data curator, developer or business developer, or share your data in our public repository Economy Data Observatory on Zenodo Join us Join our open collaboration Economy Data Observatory team as a data …","date":1623178200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623178200,"objectID":"5f2e5a9c56b7d4c024cfc4bce9a32290","permalink":"https://competition.dataobservatory.eu/post/2021-06-08-data-curator-karel-volckaert/","publishdate":"2021-06-08T18:50:00Z","relpermalink":"/post/2021-06-08-data-curator-karel-volckaert/","section":"post","summary":"Credibility is enhanced through cross-links between different data from different domains that “does not disprove” one another or that is internally consistent. If, say, data on taxable income goes in one direction and taxes in another, it is the reasoned reconciliation of the - alleged or real - inconsistency that will validate the comprehensive data set. So I am a great believer in broad, real-time observatories where not only the data capture, but the data reconciliation is automated, sometimes by means of a simple comparative statics analysis, in other cases maybe through quite elaborate artificial intelligence.","tags":["open-data","open-science","reproducible research","open government"],"title":"Credibility is Enhanced Through Cross Links Between Different Data from Different Domains","type":"post"},{"authors":["Suzan Sidal"],"categories":null,"content":"As a consultant, what type of data do you usually use in your work at ECORYS?\nWe work with a great variety of data – both from qualitative and quantitative sources – that we retrieve from publicly available sources or get through our clients. Since we are a public policy consultancy, most of the datasets are related to government reports, policies, statistics or surveys that we analyse and assess within a specific timeframe. Oftentimes, we gather open data like non-textual or numeric, such as maps and satellite images; so-called “raw data,” like weather, geospatial and environmental data; or data such as that generated in research like genomes, medical data, mathematical and scientific formulas.\nIf you were able to pick, what would be the ultimate dataset, or datasets that you would like to see in the Green Deal Data Observatory?\nI would like to see more data on the consequences and impact of increasing drought and urban heat in our cities in the Green Deal Data Observatory. Because of the complexity of rapidly developing metropolitan regions and the uncertainty associated with climate change, we need to explore more climate change adaptation and mitigation activities, or disaster risk reduction, not only climate change itself.\nSee our drought case study on how we combine very different data in our observatory We need more reliable datasets on the effect of global warming on urban resilience and more indicators to inform stakeholders on disaster risk reduction. The Green Deal Observatory could build indexes for public and private entities once we would have all the relevant data at hand. With this project, we could explore many possibilities to actually utilise open data for a common and societal good, working towards a great social cause.\nWhy did you decide to join the challenge and why do you think that this would be a game changer for policymakers and for business?\nAs a consultant for many socially relevant projects, everyday I see the importance of high quality and diverse datasets. I joined the challenge to contribute to significant causes enabled through the Green Deal Data Observatory and Economy Data Observatory. We can all benefit from the usage of open data, which is, in my opinion, a prerequisite for open government partnerships.\nI believe that through our work and through open data collaborations, we show a good example for a cultural change in the relationship between citizens and the state, which can contribute to more transparency, more participation and more intensive cooperation.\nThe access and analysis of open data for the general public would make political action more transparent and more comprehensible. This can lead to greater accountability and a sense of duty on the part of public officials to the general public, which in turn can lead to greater acceptance of government action and strengthen the public’s trust in their government and administration.\nIs there a number that recently surprised you? What was it?\nClimate change is increasing people’s exposure to heat. Extreme temperature events have been documented to be rising in frequency, duration, and magnitude over the world. The number of persons exposed to heatwaves grew by roughly 125 million between 2000 and 2016.\nSydney by Marek Piwnicki Unplash License From your experience, what do you think the greatest problem with open data in 2021 will be?\nI see two great problems with the use of open data. The first one is the low level of exploitation. The other is the lack of transparency in data processing.\nThe use of open data should be transparent and meet high quality standards. If we want to enable communities to use it for solving local problems, we must do two things. First, data must be made easy to use (or actionable), and second, we have to increase public awareness and offer training for use. Furthermore, governments should release data in usable formats that follow open data guidelines. Currently, there is very little effort made at the community level to encourage the reuse of public data for the public good.\nWhat can our automated data observatories do to make open data more credible in the European economic policy community and be more accepted as verified information?\nAlmost nothing is being done to help communities build the capability to analyze and implement open data without relying on technology.\nOur API contains rich processing and descriptive metadata besides our high-quality indicators. This is a critical task that the our fledlging data Observatories, the Digital Music Observatory, Green Deal Data Observatory and Economy Data Observatory, may be able to help with. Facilitating private-public partnerships is one step to encourage the data community to work with valuable open data. However, transparency and a high level quality assurance step must be given. In a joint collaboration with data curators, developers, technical specialists and academics, the datasets should be retrieved, cleaned and assessed in order …","date":1623096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623096000,"objectID":"47969846b175aeb1e3d60a041c8be135","permalink":"https://competition.dataobservatory.eu/post/2021-06-07-introducing-suzan-sidal/","publishdate":"2021-06-07T20:00:00Z","relpermalink":"/post/2021-06-07-introducing-suzan-sidal/","section":"post","summary":"Facilitating private-public partnerships is one step to encourage the data community to work with valuable open data. However, transparency and a high level quality assurance step must be given. In a joint collaboration with data curators, developers, technical specialists and academics, the datasets should be retrieved, cleaned and assessed in order to deliver efficient, relevant and credible information. The constant monitoring and regulation as well as compliance with data security guidelines are indispensable.","tags":["open-data","open-science","reproducible research","business case","urban heat","climate change mitigation"],"title":"Our Datasets Should be Retrieved Cleaned and Assessed in Order to Deliver Efficient Relevant and Credible Information","type":"post"},{"authors":["Pyry Kantanen"],"categories":null,"content":"As a developer at rOpenGov, and as an economic sociologist, what type of data do you usually use in your work?\nGenerally speaking, people’s access to (or inequalities in accessing) different types of resources and their ability in transforming these resources to other types of resources is what interests me. The data I usually work with is the kind of data that is actually nicely covered by existing rOpenGov tools: data about population demographics and administrative units from Statistics Finland, statistical information on welfare and health from Sotkanet and also data from Eurostat. Aside from these a lot of information is of course data from surveys and texts scraped from the internet.\nWe are placing the growing number of rOpenGov tools in a modern application with a user-friendly service and a modern data API. In your ideal data world, what would be the ultimate dataset, or datasets that you would like to see in the Music Data Observatory?\nLate spring and early summer time is, at least for me, defined by the Eurovision Song Contest. Every year watching the contest makes me ponder the state of the music industry in my home country Finland as well as in Europe. Was the song produced by homegrown talent or was it imported? Was it better received by the professional jury or the public? How well does the domestic appeal of an artist translate to the international stage? Many interesting phenomena are difficult to quantify in a meaningful way and writing a catchy song with international appeal is probably more an art than a science. Nevertheless that should not deter us from trying as music, too, is bound by certain rules and regularities that can be researched.\nMusic, too, is bound by certain rules and regularities that can be researched. Our Digital Music Observatory and its Listen Local experimental App does this exactly, and we would love to create Eurovision musicology datasets. Photo: Eurovision Song Contest 2021 press photo by Jordy Brada Why did you decide to join the EU Datathon challenge team and why do you think that this would be a game changer for researchers and policymakers?\nThe challenge has, in my opinion, great potential in leading by example when it comes to open data access and reproducible research. Comparing data to oil is a common phrase but fitting in the sense that crude oil has to go through a number of steps and pipes before it becomes useful. Most users and especially policymakers appreciate ease-of-use of the finished product, but the quality of the product and the process must also be guaranteed somehow. Openness and peer-review practices are the best guarantors in the field of data, just as industrial standards and regulations are in the oil industry.\nWe provide many layers of fully transparent quality control about the data we are placing in our data APIs and provide for our end-users. Join us Join our open collaboration Economy Data Observatory team as a data curator, developer or business developer. More interested in environmental impact analysis? Try our Green Deal Data Observatory team! Or your interest lies more in data governance, trustworthy AI and other digital market problems? Check out our Digital Music Observatory team!\n","date":1623060000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623060000,"objectID":"fdb475346ab7d01f9ead6530e48ade00","permalink":"https://competition.dataobservatory.eu/post/2021-06-07-data-curator-pyry-kantanen/","publishdate":"2021-06-07T10:00:00Z","relpermalink":"/post/2021-06-07-data-curator-pyry-kantanen/","section":"post","summary":"Many interesting phenomena are difficult to quantify in a meaningful way and writing a catchy song with international appeal is probably more an art than a science. Nevertheless that should not deter us from trying as music, too, is bound by certain rules and regularities that can be researched.","tags":["open-data","open-science","reproducible research","open government","Eurovision","musicology"],"title":"Comparing Data to Oil is a Cliché: Crude Oil Has to Go Through a Number of Steps and Pipes Before it Becomes Useful","type":"post"},{"authors":["Leo Lahti"],"categories":null,"content":"As a developer at rOpenGov, what type of data do you usually use in your work?\nAs an academic data scientist whose research focuses on the development of general-purpose algorithmic methods, I work with a range of applications from life sciences to humanities. Population studies play a big role in our research, and often the information that we can draw from public sources - geospatial, demographic, environmental - provides invaluable support. We typically use open data in combination with sensitive research data but some of the research questions can be readily addressed based on open data from statistical authorities such as Statistics Finland or Eurostat.\nIn your ideal data world, what would be the ultimate dataset, or datasets that you would like to see in the Music Data Observatory?\nOne line of our research analyses the historical trends and spread of knowledge production, in particular book printing based on large-scale metadata collections. It would be interesting to extend this research to music, to understand the contemporary trends as well as the broader historical developments. Gaining access to a large systematic collection of music and composition data from different countries across long periods of time would make this possible.\nWhy did you decide to join the challenge and why do you think that this would be a game changer for researchers and policymakers?\nJoining the challenge was a natural development based on our overall activities in this area; the rOpenGov project has been around for a decade now, since the early days of the broader open data movement. This has also created an active international developer network and we felt well equipped for picking up the challenge. The game changer for researchers is that the project highlights the importance of data quality, even when dealing with official statistics, and provides new methods to solve these issues efficiently through the open collaboration model. For policymakers, this provides access to new high-quality curated data and case studies that can support evidence-based decision-making.\nDo you have a favorite, or most used open governmental or open science data source? What do you think about it? Could it be improved?\nRegarding open government data, one of my favorites is not a single data source but a data representation standard. The px format is widely used by statistical authorities in various countries, and this has allowed us to create R tools that allow the retrieval and analysis of official statistics from many countries across Europe, spanning dozens of statistical institutions. Standardization of open data formats allows us to build robust algorithmic tools for downstream data analysis and visualization. Open government data is still too often shared in obscure, non-standard or closed-source file formats and this is creating significant bottlenecks for the development of scalable and interoperable AI and machine learning methods that can harness the full potential of open data.\nRegarding open government data, one of my favorites is not a single data source but a data representation standard, the Px format. From your perspective, what do you see being the greatest problem with open data in 2021?\nAlthough there are a variety of open data sources available (and the numbers continue to increase), the availability of open algorithmic tools to interpret and communicate open data efficiently is lagging behind. One of the greatest challenges for open data in 2021 is to demonstrate how we can maximize the potential of open data by designing smart tools for open data analytics.\nWhat can our automated data observatories do to make open data more credible in the European economic policy community and be accepted as verified information?\nThe role of the professional network backing up the project, and the possibility of getting critical feedback and later adoption by the academic communities will support the efforts. Transparency of the data harmonization operations is the key to credibility, and will be further supported by concrete benchmarks that highlight the critical differences in drawing conclusions based on original sources versus the harmonized high-quality data sets.\nWe need to get critical feedback and later adoption by the academic communities. How we can ensure the long-term sustainability of the efforts?\nThe extent of open data space is such that no single individual or institution can address all the emerging needs in this area. The open developer networks play a huge role in the development of algorithmic methods, and strong communities have developed around specific open data analytical environments such as R, Python, and Julia. These communities support networked collaboration and provide services such as software peer review. The long-term sustainability will depend on the support that such developer communities can receive, both from individual contributors as well as from institutions and governments.\nJoin our open …","date":1622800800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622800800,"objectID":"fcf382863b28f6f8f721290cf2d05608","permalink":"https://competition.dataobservatory.eu/post/2021-06-04-developer-leo-lahti/","publishdate":"2021-06-04T10:00:00Z","relpermalink":"/post/2021-06-04-developer-leo-lahti/","section":"post","summary":"Although there are a variety of open data sources available (and the numbers continue to increase), the availability of open algorithmic tools to interpret and communicate open data efficiently is lagging behind. One of the greatest challenges for open data in 2021 is to demonstrate how we can maximize the potential of open data by designing smart tools for open data analytics.","tags":["open-data","open-science","reproducible research","open government","R"],"title":"Creating Algorithmic Tools to Interpret and Communicate Open Data Efficiently","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"We have released a new version of iotables as part of the rOpenGov project. The package, as the name suggests, works with European symmetric input-output tables (SIOTs). SIOTs are among the most complex governmental statistical products. They show how each country’s 64 agricultural, industrial, service, and sometimes household sectors relate to each other. They are estimated from various components of the GDP, tax collection, at least every five years.\nSIOTs offer great value to policy-makers and analysts to make more than educated guesses on how a million euros, pounds or Czech korunas spent on a certain sector will impact other sectors of the economy, employment or GDP. What happens when a bank starts to give new loans and advertise them? How is an increase in economic activity going to affect the amount of wages paid and and where will consumers most likely spend their wages? As the national economies begin to reopen after COVID-19 pandemic lockdowns, is to utilize SIOTs to calculate direct and indirect employment effects or value added effects of government grant programs to sectors such as cultural and creative industries or actors such as venues for performing arts, movie theaters, bars and restaurants.\nMaking such calculations requires a bit of matrix algebra, and understanding of input-output economics, direct, indirect effects, and multipliers. Economists, grant designers, policy makers have those skills, but until now, such calculations were either made in cumbersome Excel sheets, or proprietary software, as the key to these calculations is to keep vectors and matrices, which have at least one dimension of 64, perfectly aligned. We made this process reproducible with iotables and eurostat on rOpenGov\nOur iotables package creates direct, indirect effects and multipliers programatically. Our observatory will make those indicators available for all European countries. Accessing and tidying the data programmatically The iotables package is in a way an extension to the eurostat R package, which provides a programmatic access to the Eurostat data warehouse. The reason for releasing a new package is that working with SIOTs requires plenty of meticulous data wrangling based on various metadata sources, apart from actually accessing the data itself. When working with matrix equations, the bar is higher than with tidy data. Not only your rows and columns must match, but their ordering must strictly conform the quadrants of the a matrix system, including the connecting trade or tax matrices.\nWhen you download a country’s SIOT table, you receive a long form data frame, a very-very long one, which contains the matrix values and their labels like this:\n## Table naio_10_cp1700 cached at C:\\Users\\...\\Temp\\RtmpGQF4gr/eurostat/naio_10_cp1700_date_code_FF.rds # we save it for further reference here saveRDS(naio_10_cp1700, \u0026#34;not_included/naio_10_cp1700_date_code_FF.rds\u0026#34;) # should you need to retrieve the large tempfiles, they are in dir (file.path(tempdir(), \u0026#34;eurostat\u0026#34;)) dplyr::slice_head(naio_10_cp1700, n = 5) ## # A tibble: 5 x 7 ## unit stk_flow induse prod_na geo time values ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 MIO_EUR DOM CPA_A01 B1G EA19 2019-01-01 141873. ## 2 MIO_EUR DOM CPA_A01 B1G EU27_2020 2019-01-01 174976. ## 3 MIO_EUR DOM CPA_A01 B1G EU28 2019-01-01 187814. ## 4 MIO_EUR DOM CPA_A01 B2A3G EA19 2019-01-01 0 ## 5 MIO_EUR DOM CPA_A01 B2A3G EU27_2020 2019-01-01 0 The metadata reads like this: the units are in millions of euros, we are analyzing domestic flows, and the national account items B1-B2 for the industry A01. The information of a 64x64 matrix (the SIOT) and its connecting matrices, such as taxes, or employment, or C**O2 emissions, must be placed exactly in one correct ordering of columns and rows. Every single data wrangling error will usually lead in an error (the matrix equation has no solution), or, what is worse, in a very difficult to trace algebraic error. Our package not only labels this data meaningfully, but creates very tidy data frames that contain each necessary matrix of vector with a key column.\niotables package contains the vocabularies (abbreviations and human readable labels) of three statistical vocabularies: the so called COICOP product codes, the NACE industry codes, and the vocabulary of the ESA2010 definition of national accounts (which is the government equivalent of corporate accounting).\nOur package currently solves all equations for direct, indirect effects, multipliers and inter-industry linkages. Backward linkages show what happens with the suppliers of an industry, such as catering or advertising in the case of music festivals, if the festivals reopen. The forward linkages show how much extra demand this creates for connecting services that treat festivals as a ‘supplier’, such as cultural tourism.\nLet’s seen an example ## Downloading employment data from the Eurostat database. ## Table lfsq_egan22d cached at …","date":1622736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622736000,"objectID":"fc5992af6c8de60c64b311b09922b493","permalink":"https://competition.dataobservatory.eu/post/2021-06-03-iotables-release/","publishdate":"2021-06-03T16:00:00Z","relpermalink":"/post/2021-06-03-iotables-release/","section":"post","summary":"rOpenGov, Reprex, and other open collaboration partners teamed up to build on our expertise of open source statistical software development further: we want to create a technologically and financially feasible data-as-service to put our reproducible research products into wider user for the business analyst, scientific researcher and evidence-based policy design communities. Our new release will help with automated economic impact and environmental impact analysis.","tags":["open-data","open-science","iotables","datathon","economic impact analysis","environmental impact analysis"],"title":"Economic and Environment Impact Analysis, Automated for Data-as-Service","type":"post"},{"authors":["Peter Ormosi"],"categories":null,"content":"As someone who’s worked in data for almost 20 years, what type of data do you usually use in your research?\nIn my field (industrial organisation, competition policy), company level financial data, and product price and sales data have been the conventional building blocks of research papers. Ideally this has been the sort of data that I would seek out for my work. Of course as academic researchers we often get knocked back by the reality of data access and availability. I would think that industrial organisation is one of those fields where researchers have to be quite innovative in terms of answering interesting and relevant policy questions, whilst having to operate in an environment where most relevant data is proprietary and very expensive. Against this backdrop, I have worked with neatly organised proprietary datasets, self-assembled data collections, and also textual data.\nFrom your experience working with various data sets, models, and frameworks, what would be the ultimate dataset, or datasets that you would like to see from the Economy Data Observatory?\nThere seems to be an emerging consensus that market concentration and markups have been continuously increasing across the economy. But most of these works use industry classification to define markets. One of the things I’d really like to see coming out of the Economy Data Observatory is a mapping of what we call antitrust markets.\nMapping NACE to Antitrust Markets. Available datasets use standard industry classification (such as NACE in the EU), which is often very different from what we call a product market in microeconomics. Product markets are defined by demand, and supply-side substitutability, which is a dynamically evolving feature and difficult to capture systematically on a wider scale. But with the recent proliferation of data and the growth (and fall in price) of computing power, I am positive that we could attempt to map out the European economy along these product market boundaries. Of course this is not without any challenge. For example in digital markets, traditional ways to define markets have caused serious challenges to competition authorities around the world.\nI believe that there is an immensely rich, and largely unexplored source of information in unstructured textual data that would be hugely useful for applied microeconomic works, including my own area of IO and competition policy. This includes a large corpus of administrative and court decisions that relate to businesses, such as merger control decisions of the European Commission. To give two examples from my experience, we’ve used a large corpus of news reports related to various firms to gauge the reputational impact of European Commission cartel investigations, or we’ve trained an algorithm to be able to classify US legislative bills and predict whether they have been lobbied or not. Finding a way to collect and convert this unstructured data into a format that is relevant and useful for users is not a trivial challenge, but is one of the most exciting parts of our Economy Data Observatory plans (see related project plan).\nFinding a way to collect and convert this unstructured data into a format that is relevant and useful for users is not a trivial challenge, but is one of the most exciting parts of our Economy Data Observatory plans. What is an idea that you consider will be a game changer for researchers and/or policymakers?\nPartly talking in the past tense, the use of data driven approaches, automation in research, and machine learning have been increasingly influential and I think this trend will continue to all areas of social science. 10 years ago, to do machine learning, you had to build your models from scratch, typically requiring a solid understanding of programming and linear algebra. Today, there are readily available deep learning frameworks like TensorFlow, Keras, PyTorch, to design a neural network for your own application. 10 years ago, natural language processing would have only been relevant for a small group of computational linguists. Today we have massive word embedding models trained on an enormous corpus of texts, at the fingertip of any researcher. 10 years ago, the cost of computing power would have made it prohibitive for most researchers to run even relatively shallow neural networks. Today, I can run complex deep learning models on my laptop using cloud computing servers. As a result of these developments, whereas 10 years ago one would have needed a small (or large) research team to explore certain research questions, much of this can now be automated and be done by a single researcher. For researchers without access to large research grants and without the ability to hire a research team, this has truly been an amazing victory for the democratisation of research.\nYou can already try out our API. Do you have a favorite, or most used open governmental or open science data source? What do you think about it? Could it be improved?\nAs a …","date":1622653200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622653200,"objectID":"90e3f33d7c682514f515b645293537eb","permalink":"https://competition.dataobservatory.eu/post/2021-06-02-data-curator-peter-ormosi/","publishdate":"2021-06-02T17:00:00Z","relpermalink":"/post/2021-06-02-data-curator-peter-ormosi/","section":"post","summary":"I believe that there is an immensely rich, and largely unexplored source of information in unstructured textual data that would be hugely useful for applied microeconomic works, including my own area of IO and competition policy.","tags":["open-data","open-science","computational antitrust","curators","economics"],"title":"New Indicators for Computational Antitrust","type":"post"},{"authors":["Daniel Antal","Botond Vitos"],"categories":null,"content":"Our observatory has a new data API which allows access to our daily refreshing open data. You can access the API via api.economy.dataobservatory.eu (apologies for the ugly, temporary subdomain masking!).\nAll the data and the metadata are available as open data, without database use restrictions, under the ODbL license. However, the metadata contents are not finalized yet. We are currently working on a solution that applies the FAIR Guiding Principles for scientific data management and stewardship, and fulfills the mandatory requirements of the Dublic Core metadata standards and at the same time the mandatory requirements, and most of the recommended requirements of DataCite. These changes will be effective before 1 July 2021.\nThe Competition Data Observatory temporarily shares an API with the Economy Data Observatory, which serves as an incubator for similar economy-oriented reproducible research resources.\nIndicator table The indicator table contains the actual values, and the various estimated/imputed values of the indicator, clearly marking missing values, too.\napi.economy.dataobservatory.eu: indicator retrieval You can get the data in CSV or json format, or write SQL querries. (Tutorials in SQL, R, Python will be posted shortly.)\nDescription metadata table Processing Metadata table The metadata table contains various data processing information, such as the first and last actual observation of the indicator, the number of approximated, forecasted, backcasted values, last update at source and in our system, and so on.\napi.economy.dataobservatory.eu: processing metadata Authoritative Copies Greendeal Data Observatory on Zenodo\n","date":1622545200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625655600,"objectID":"1fa58637f02a8941c5b61274a0207f37","permalink":"https://competition.dataobservatory.eu/data/api/","publishdate":"2021-06-01T11:00:00Z","relpermalink":"/data/api/","section":"data","summary":"Get data from the Competition Data Observatory via our API","tags":["api"],"title":"Data API","type":"data"},{"authors":["Daniel Antal"],"categories":null,"content":"Our observatory has a new data API which allows access to our daily refreshing open data. You can access the API via api.economy.dataobservatory.eu (apologies for the ugly, temporary subdomain masking!).\nAll the data and the metadata are available as open data, without database use restrictions, under the ODbL license. However, the metadata contents are not finalized yet. We are currently working on a solution that applies the FAIR Guiding Principles for scientific data management and stewardship, and fulfills the mandatory requirements of the Dublic Core metadata standards and at the same time the mandatory requirements, and most of the recommended requirements of DataCite. These changes will be effective before 1 July 2021.\nThe Competition Data Observatory temporarily shares an API with the Economy Data Observatory, which serves as an incubator for similar economy-oriented reproducible research resources.\napi.economy.dataobservatory.eu: processing metadata Descriptive Metadata Identifier An unambiguous reference to the resource within a given context. (Dublin Core item), but several identifiders allowed, and we will use several of them. Creator The main researchers involved in producing the data, or the authors of the publication, in priority order. To supply multiple creators, repeat this property. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) Title A name given to the resource. Extends Dublin Core with alternative title, subtitle, translated Title, and other title(s). Publisher The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. (Dublin Core item.) Publication Year The year when the data was or will be made publicly available. Resource Type We publish Datasets, Images, Report, and Data Papers. (Dublin Core item with controlled vocabulary.) Recommended for discovery The Recommended (R) properties are optional, but strongly recommended for interoperability.\nSubject The topic of the resource. (Dublin Core item.) Contributor The institution or person responsible for collecting, managing, distributing, or otherwise contributing to the development of the resource. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) When applicable, we add Distributor (of the datasets and images), Contact Person, Data Collector, Data Curator, Data Manager, Hosting Institution, Producer (for images), Project Manager, Researcher, Research Group, Rightsholder, Sponsor, Supervisor Date A point or period of time associated with an event in the lifecycle of the resource, besides the Dublin Core minimum we add Collected, Created, Issued, Updated, and if necessary, Withdrawn dates to our datasets. Related Identifier An identifier or identifiers other than the primary Identifier applied to the resource being registered. Rights We give SPDX License List standards rights description with URLs to the actual license. (Dublin Core item: Rights Management) Description Recommended for discovery.(Dublin Core item.) GeoLocation Similar to Dublin Core item Coverage The Subject property: we need to set standard coding schemas for each observatory. Contributor property: DataCurator the curator of the dataset, who sets the mandatory properties. DataManager the person who keeps the dataset up-to-date. ContactPerson the person who can be contacted for reuse requests or bug reports. The Date property contains the following dates, which are set automatically by the dataobservatory R package: Updated when the dataset was updated; EarliestObservation, which the earliest, not backcasted, estimated or imputed observation. LatestObservation, which the earliest, not backcasted, estimated or imputed observation. UpdatedatSource, when the raw data source was last updated. The GeoLocation is automatically created by the dataobservatory R package. The Description property optional elements, and we adopted them as follows for the observatories: The Abstract is a short, textual description; we try to automate its creation as much as a possible, but some curatorial input is necessary. In the TechnicalInfo sub-field, we record automatically the utils::sessionInfo() for computational reproducability. This is automatically created by the dataobservatory R package. In the Other sub-field, we record the keywords for structuring the observatory. Optional The Optional (O) properties are optional and provide richer description. For findability they are not so important, but to create a web service, they are essential. In the mandatory and recommended fields, we are following other metadata standards and codelists, but in the optional fields we have to build up our own system for the observatories.\nLanguage A language of the resource. (Dublin Core item.) Alternative …","date":1622545200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625735400,"objectID":"882a6a70ec8965c6a0246cec45271324","permalink":"https://competition.dataobservatory.eu/data/metadata/","publishdate":"2021-06-01T11:00:00Z","relpermalink":"/data/metadata/","section":"data","summary":"Uncut diamonds need to be cut, polished, and you have to make sure that they come from a legal source.","tags":["metadata"],"title":"Metadata","type":"data"},{"authors":null,"categories":null,"content":"Big data and AI create new inequalities in the world. Data monopolization can potentially lead to product and service market monopolization, too. We are designing new policy indicators that can help competition policy practitioners and researchers to measure and understand these trends.\nWe are planning to programatically access the merger database of the European Commission’s Merger Cases, which we believe to fall under the scope of the Open Data Directive. We would like to connect to the EUIPO databases to create indicators of potential killer acquisitions - acquisitions by large companies that may thwart innovation by buying up and killing start-up companiees to take hold of competitive patents. Mapping antitrust markets Most market-based analysis uses standardised industry classifications such as the NACE system in the European Union. Whilst these systems are useful to have a standardised view of the component parts of the economy, these markets are frequently very different from product markets that are defined by demand and supply conditions (literature refers to this as antitrust markets). One of the cornerstones of our Data Observatory is automating the construction and updating of a system of antitrust markets. This can be done from available antitrust market decisions. For example Affeldt et al. (2021) manually identified 20,000 product/geographic antitrust markets affected by over 2,000 mergers from DG COMP merger decisions. Automating this process will create a continuously updated source of market definitions that can directly benefit a number of users, such as small businesses caught up in costly merger litigations, academics, policy organisations. This new market classification system could also contribute to the topical debate on increasing market concentration and markups (where most current analysis uses standard industry classification systems). Under the umbrella of our Antitrust Market Classification (AMC) and under a standard industry classification (NACE) we will create three company-level tracker datasets:\nComputational Antitrust Project Plan Merger tracker We compile a merger database from constructing our antitrust markets. This will be complimented by continuously monitoring markets for M\u0026amp;A transactions that are below the radar of DG COMP. As a first step, we will only have this option for selected markets, until we develop a tool to systematically collect this data on a wider scope. Tracking mergers in both NACE and AMC markets helps trace how concentration evolves over time. Moreover, this is a key building block to our Competition and Innovation Data Observatory.\nInnovation tracker Part of this project is to link the firms that appear in our antitrust market definitions to their historical and current innovation data. For this we will draw from patent data APIs, such as that of the European Patent Office. This would enable the tracking of how innovation develops as market become more/less concentrated. This is important for several reasons. First of all, innovation is an important component of economic productivity, and therefore our tracker will provide vital information to competition authorities and sectoral regulators. Second, there are increasingly vocal concerns about the innovation impact of acquisitions between companies that are either on separate antitrust markets, or include an acquired business that is below the size threshold of antitrust scrutiny (these are frequently referred to as killer acquisitions in the literature). One side of the ongoing debate argues that these acquisitions can hamper innovation, whereas other claim the opposite (it is the prospect of these acquisitions that drives innovation in many small start-up businesses). As such, our tool will be a key indicator of how healthy the environment for innovation for small businesses is.\nOwnership tracker Finally, we will construct and ownership tracker from the shareholder reports of listed companies (from the set of companies identified in our market definition process). Once again, this is key information for gauging how healthy competition is in European markets. It has been repeatedly highlighted that overlaps in the ownership structure (in institutional investors) of the largest businesses can hinder competition. Mapping the ownership structure of European listed companies means that not only we are aware of market concentration concerns in antitrust markets, but also concerns when overlap across corporate governance structures is considered.\nPhoto credit: Matt Ridley, Unplash licence.\n","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621123200,"objectID":"e44d98b55e5e2f27165014a2238b5b61","permalink":"https://competition.dataobservatory.eu/project/competition/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/project/competition/","section":"project","summary":"Mapping data between antitrust and economic activity markets","tags":["competition","competitiveness","innovation"],"title":"Competition","type":"project"},{"authors":null,"categories":null,"content":"we would like to actively encourage the sharing of data assets.\n","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621123200,"objectID":"ebd4557c0a6e81227426e984ce16d9a4","permalink":"https://competition.dataobservatory.eu/data/data-sharing/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/data/data-sharing/","section":"data","summary":"Data altruismm, sharing, and collaborative data resources.","tags":["data-sharing","data-altruism"],"title":"Data Sharing","type":"data"},{"authors":null,"categories":null,"content":"Many countries in the world allow access to a vast array of information, such as documents under freedom of information requests, statistics, datasets. In the European Union, most taxpayer financed data in government administration, transport, or meteorology, for example, can be usually re-used. More and more scientific output is expected to be reviewable and reproducible, which implies open access.\nWhat’s the Problem with Open Data? How We Add Value? Is There Value in It? If it’s money on the street, why nobody’s picking it up? Datasets Should Work Together to Give InformationData is only potential information, raw and unprocessed. What’s the Problem with Open Data? “Data is stuff. It is raw, unprocessed, possibly even untouched by human hands, unviewed by human eyes, un-thought-about by human minds.” [1]\nMost open data cannot be just “downloaded.” Often, you need to put more than $100 value of work into processing, validating, documenting a dataset that is worth $100. But you can share this investment with our data observatories. Open data is almost always lacking of documentation, and no clear references to validate if the data is reliable or not corrupted. This is why we always start with reprocessing and redocumenting. Our review of about 80 EU, UN and OECD data observatories reveals that most of them do not use these organizations’s open data - instead they use various, and often not well processed proprietary sources. Read more: Open Data - The New Gold Without the Rush\nHow We Add Value? We believe that even such generally trusted data sources as Eurostat often need to be reprocessed, because various legal and political constraints do not allow the common European statistical services to provide optimal quality data – for example, on the regional and city levels. With rOpenGov and other partners, we are creating open-source statistical software in R to re-process these heterogenous and low-quality data into tidy statistical indicators to automatically validate and document it. Metadata is a potentially informative data record about a potentially informative dataset. We are carefully documenting and releasing administrative, processing, and descriptive metadata, following international metadata standards, to make our data easy to find and easy to use for data analysts. We are automatically creating depositions and authoritative copies marked with an individual digital object identifier (DOI) to maintain data integrity. Is There Value in Open Data? A well-known story tells of a finance professor and a student who come across a $100 bill lying on the ground. As the student stops to pick it up, the professor says, “Don’t bother—if it were really a $100 bill, it wouldn’t be there.”\nBut this is not the case with open data. Often, you need to put more than $100 into processing, validating, documenting a dataset that is worth $100.\nIn the EU, open data is governed by the Directive on open data and the re-use of public sector information - in short: Open Data Directive (EU) 2019 / 1024. It entered into force on 16 July 2019. It replaces the Public Sector Information Directive, also known as the PSI Directive which dated from 2003 and was subsequently amended in 2013.\nOpen Data is potentially useful data that can potentially replace costlier or hard to get data sources to build information. It is analogous to potential energy: work is required to release it. We build automated systems that reduce this work and increase the likelihood that open data will offer the best value for money.\nMost open data is not publicy accessible, and available upon request. Our real curatorial advantage is that we know where it is and how to get this request processed. Most European open data comes from tax authorities, meteorological offices, managers of transport infrastructure, and other governmental bodies whose data needs are very different from yours. Their data must be carefully evaluated, re-processed, and if necessary, imputed to be usable for your scientific, business or policy goals. The use of open science data is problematic in different ways: usually understanding the data documentation requires domain-specific specialist knowledge. Open science data is even more scattered and difficult to access than technically open, but not public governmental data. From Datasets to Data Integration, Data to Information “Data is only potential information, raw and unprocessed, prior to anyone actually being informed by it.” ^[2]\nWe are building simple databases and supporting APIs that release the data without restrictions, in a tidy format that is easy to join with other data, or easy to join into databases, together with standardized metadata. Our service flow and value chain FAQ Why Downloading Does Not Work? Most open data is not available on the internet. If it is available, it is not in a form that you can easily import into a spreadsheet application like Excel or OpenOffice, or into a statistical application like SPSS or …","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624633200,"objectID":"c02f0e35bf46fd958fd3ec2bd0a929fb","permalink":"https://competition.dataobservatory.eu/data/open-gov/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/data/open-gov/","section":"data","summary":"Many countries in the world allow access to a vast array of information, such as documents under freedom of information requests, statistics, datasets. In the European Union, most taxpayer financed data in government administration, transport, or meteorology, for example, can be usually re-used. More and more scientific output is expected to be reviewable and reproducible, which implies open access.","tags":["open-data","FOI","PSI"],"title":"Open Data","type":"data"},{"authors":null,"categories":null,"content":"Photo credit: Tim Mossholder, Unplash licence.\n","date":1620777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620777600,"objectID":"9262e474391442420de8705e59360b01","permalink":"https://competition.dataobservatory.eu/project/sme/","publishdate":"2021-05-12T00:00:00Z","relpermalink":"/project/sme/","section":"project","summary":"Access to finance, activity, and regulatory hurdles indicators","tags":["SME","microenterprise","open-data","open government","open science"],"title":"Small- and Medium Sized Enterprizes","type":"project"},{"authors":null,"categories":null,"content":"Reprex, a Dutch start-up enterprise formed to utilize open source software and open data, is looking for partners in an agile, open collaboration to win at least one of the three EU Datathon Prizes. We are looking for policy partners, academic partners and a consultancy partner. Our project is based on agile, open collaboration with three types of contributors.\nWith our competing prototypes we want to show that we have a research automation technology that can find open data, process it and validate it into high-quality business, policy or scientific indicators, and release it with daily refreshments in a modern API.\nWe are looking for institutions to challenge us with their data problems, and sponsors to increase our capacity. Over then next 5 months, we need to find a sustainable business model for a high-quality and open alternative to other public data programs.\nThe EU Datathon 2021 Challenge To take part, you should propose the development of an application that links and uses open datasets. - our data curator team\nYour application … is also expected to find suitable new approaches and solutions to help Europe achieve important goals set by the European Commission through the use of open data.” - this application is developed by our technology contributors\nYour application should showcase opportunities for concrete business models or social enterprises. - our service development team is working to make this happen!\nWe use open source software and open data. The applications are hosted on the cloud resources of Reprex, an early-stage technology startup currently building a viable, open-source, open-data business model to create reproducible research products.\nWe are working together with experts in the domain as curators (check out our guidelines if you want to join: Data Curators: Get Inspired!).\nOur development team works on an open collaboration basis. Our indicator R packages, and our services are developed together with rOpenGov.\nMission statement We want to win an EU Datathon prize by processing the vast, already-available governmental and scientific open data made usable for policy-makers, scientific researchers, and business researcher end-users.\n“To take part, you should propose the development of an application that links and uses open datasets. Your application should showcase opportunities for concrete business models or social enterprises. It is also expected to find suitable new approaches and solutions to help Europe achieve important goals set by the European Commission through the use of open data.”\nWe aim to win at least one first prize in the EU Datathon 2021. We are contesting all three challenges, which are related to the EU’s official strategic policies for the coming decade.\nChallenge 1: A European Grean Deel Our Green Deal Data Observatory connects socio-economic and environmental data to help understanding and combating climate change. Challenge 1: A European Green Deal, with a particular focus on the The European Climate Pact, the Organic Action Plan, and the New European Bauhaus, i.e., mitigation strategies.\nClimate change and environmental degradation are an existential threat to Europe and the world. To overcome these challenges, the European Union created the European Green Deal strategic plan, which aims to make the EU’s economy sustainable by turning climate and environmental challenges into opportunities and making the transition just and inclusive for all.\nOur Green Deal Data Observatory is a modern reimagination of existing ‘data observatories’; currently, there are over 70 permanent international data collection and dissemination points. One of our objectives is to understand why the dozens of the EU’s observatories do not use open data and reproducible research. We want to show that open governmental data, open science, and reproducible research can lead to a higher quality and faster data ecosystem that fosters growth for policy, business, and academic data users.\nWe provide high quality, tidy data through a modern API which enables data flows between public and proprietary databases. We believe that introducing Open Policy Analysis standards with open data, open-source software, and research automation, can help the Green Deal policymaking process. Our collaboration is open for individuals, citizens scientists, research institutes, NGOS, and companies.\nChallenge 2: An economy that works for people Our Economy Data Observatory will focus on competition, small and medium sized enterprizes and robotization. Challenge 2: An economy that works for people, with a particular focus on the Single market strategy, and particular attention to the strategy’s goals of 1. Modernising our standards system, 2. Consolidating Europe’s intellectual property framework, and 3. Enabling the balanced development of the collaborative economy strategic goals.\nBig data and automation create new inequalities and injustices and have the potential to create a jobless growth economy. Our …","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"50c9aae83f8b9495d626ec53825d6ffc","permalink":"https://competition.dataobservatory.eu/project/eu-datathon_2021/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/project/eu-datathon_2021/","section":"project","summary":"Reprex, a Dutch start-up enterprise formed to utilize open source software and open data, is looking for partners in an agile, open collaboration to win at least one of the three EU Datathon Prizes.","tags":["data-observatory","reproducible-research","open-data","open government","open science"],"title":"EU Datathon 2021","type":"project"},{"authors":["Competition Data Observatory","Daniel Antal"],"categories":null,"content":"In our use case we are merging data about Europe’s coal regions, harmonized surveys about the acceptance of climate policies, and socio-economic data. While the work starts out from existing European research, our retroharmonize survey harmonization solution, our regions sub-national boundary harmonization solution and iotables allows us to connect open data and open knowledge from other coal regions of the world, for example, from the Appalachian economy.\nPolicy Context The Just Transition Platform aims to assist EU countries and regions to unlock the support available through the Just Transition Mechanism. It builds on and expands the work of the existing Initiative for Coal Regions in Transition, which already supports fossil fuel producing regions across the EU in achieving a just transition through tailored, needs-oriented assistance and capacity-building.\nThe Initiative has a secretariat that is co-run by Ecorys, Climate Strategies, ICLEI Europe, and the Wuppertal Institute for Climate. While the initiative is an EU project, it cooperates with other similar initiatives, for example, with the Coalfield Development social enterprise in the Appalachian economy.\nData Sources Coal regions: Our starting point is the EU coal regions: opportunities and challenges ahead publication Joint Research Centre (JRC), the European Commission’s science and knowledge service. This publication maps Europe’s coal dependent energy and transport infrastructure, and regions that depend on coal-related jobs.\nHarmonized Survey Data: The dataset of the Eurobarometer 91.3 (April 2019) harmonized survey. Our transition policy variable is the four-level agreement with the statement More public financial support should be given to the transition to clean energies even if it means subsidies to fossil fuels should be reduced (EN) and Davantage de soutien financier public devrait être donné à la transition vers les énergies propres même si cela signifie que les subventions aux énergies fossiles devraient être réduites (FR) which is then translated to the language use of all participating country.\nEnvironmental Variables: We used data on pm and SO2 polution measured by participating stations in the European Environmental Agency’s monitoring program. The station locations were mapped by Milos to the NUTS sub-national regions.\nExploratory Data Analysis Our coal-dependency dummy variable is base on the policy document Coal regions in transition.\nreadRDS(file.path(\u0026#34;data\u0026#34;, \u0026#34;coal_regions.rds\u0026#34;)) ## # A tibble: 253 x 5 ## country_code_is~ region_nuts_nam~ region_nuts_cod~ coal_region is_coal_region ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 BE Brussels hoofds~ BE10 \u0026lt;NA\u0026gt; 0 ## 2 BE Liege BE33 \u0026lt;NA\u0026gt; 0 ## 3 BE Brabant Wallon BE31 \u0026lt;NA\u0026gt; 0 ## 4 BE Antwerpen BE21 \u0026lt;NA\u0026gt; 0 ## 5 BE Limburg [BE] BE22 \u0026lt;NA\u0026gt; 0 ## 6 BE Oost-Vlaanderen BE23 \u0026lt;NA\u0026gt; 0 ## 7 BE Vlaams Brabant BE24 \u0026lt;NA\u0026gt; 0 ## 8 BE West-Vlaanderen BE25 \u0026lt;NA\u0026gt; 0 ## 9 BE Hainaut BE32 \u0026lt;NA\u0026gt; 0 ## 10 BE Namur BE35 \u0026lt;NA\u0026gt; 0 ## # ... with 243 more rows Our exploratory data analysis shows that respondent in 2019, agreement with the policy measure significantly differed among EU member states and regions.\ntransition_policy \u0026lt;- eb19_raw %\u0026gt;% rowid_to_column() %\u0026gt;% mutate ( transition_policy = normalize_text(transition_policy)) %\u0026gt;% fastDummies::dummy_cols(select_columns = \u0026#39;transition_policy\u0026#39;) %\u0026gt;% mutate ( transition_policy_agree = case_when( transition_policy_totally_agree + transition_policy_tend_to_agree \u0026gt; 0 ~ 1, TRUE ~ 0 )) %\u0026gt;% mutate ( transition_policy_disagree = case_when( transition_policy_totally_disagree + transition_policy_tend_to_disagree \u0026gt; 0 ~ 1, TRUE ~ 0 )) eb19_df \u0026lt;- transition_policy %\u0026gt;% left_join ( air_pollutants, by = \u0026#39;region_nuts_codes\u0026#39; ) %\u0026gt;% mutate ( is_poland = ifelse ( country_code == \u0026#34;PL\u0026#34;, 1, 0)) Preliminary Results Significantly more people agree where\nthere are more polutants who are younger where people are more educated Significantly less people agree\nin rural areas where more people are older where more people are less educated in less polluted areas in coal regions A simple model run:\nc(\u0026#34;transition_policy_totally_agree\u0026#34; , \u0026#34;pm10\u0026#34;, \u0026#34;so2\u0026#34;, \u0026#34;age_exact\u0026#34;, \u0026#34;is_highly_educated\u0026#34; , \u0026#34;is_rural\u0026#34;) ## [1] \u0026#34;transition_policy_totally_agree\u0026#34; \u0026#34;pm10\u0026#34; ## [3] \u0026#34;so2\u0026#34; \u0026#34;age_exact\u0026#34; ## [5] \u0026#34;is_highly_educated\u0026#34; \u0026#34;is_rural\u0026#34; summary( glm ( transition_policy_totally_agree ~ pm10 + so2 + age_exact + is_highly_educated + is_rural + is_coal_region + country_code, data = eb19_df, family = binomial )) ## ## Call: ## glm(formula = transition_policy_totally_agree ~ pm10 + so2 + ## age_exact + is_highly_educated + is_rural + is_coal_region + ## country_code, family = binomial, data = eb19_df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7690 -1.0253 -0.8165 1.2264 1.9085 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.1975096 0.0921551 -2.143 0.032095 * ## pm10 0.0068505 0.0017445 3.927 8.60e-05 *** ## so2 0.1381994 0.0405867 3.405 0.000662 *** ## age_exact -0.0075018 0.0007873 -9.529 \u0026lt; 2e-16 *** ## …","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"76bbab8f6e17ec9b2c049461abf4b1e3","permalink":"https://competition.dataobservatory.eu/publication/political-roadblocks/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/publication/political-roadblocks/","section":"publication","summary":"Our first use case is about identifying potential political roadblocks for climate policies.  We are combining survey data about attitudes to climate change policies with socio-economic coal mining and and voting data.  We examine the relationship between voter attitudes and economic dependency on coal mining.","tags":["coal","use-case","regions"],"title":"Identifying Roadblocks to Net Zero Legislation","type":"publication"},{"authors":[],"categories":null,"content":"Open Data Day is an annual celebration of open data all over the world. It is an opportunity to show the benefits of open data and encourage the adoption of open data policies in government, business, and civil society. Reprex is a start-up that utilizes open data with open-source reproducible research: please challenge us with your data requests and participate in our web events.\nThe Reprex Open Data Day 2021 will be two informal conversations based on a series of run up introductory blogposts centered around two themes. Because important guests became ill in the last days, we are going to consolidate the two talks into one with less structure. We want to create an informal, inclusive, collaborative online event on International Open Data Day 2021. Please, grab a tea, coffee, or even a beer, and join us for an informal conversation. We hope that we will finish the afternoon with ideas on new, open-data driven collaborations.\n9.30 EST / 15.30 CET: Open collaboration in business, policy and science. Creating evidence-based policy, business strategy or scientific research with small contributions with independent components with incentives. Short introduction with examples: joining environmental sensory data and public opinion data on maps; creating harmonized datasets across the Arab world. Survey harmonization, mapping, data products. Scaling up open collaboration: making small organizations competitive with big tech in the big data era. Data sharing, data pooling, data altruism and observatories. The new European trustworthy AI and data governance agenda.\nYou can click through a short presentation to familiarize yourself with our topics.\nSee you here.\nCase studies:\nWe are connecting raw survey data about Climate Awareness in Eurobarometer surveys. Here is the reproduction code (intermediate to advanced R needed.) You should use the development version of our retroharmonize package at github.com/antaldaniel/retroharmonize\nWe are tracking changes in the boundaries of provinces, states, counties, parishes with our regions open source software – reproduction code here. You will need our regions package which is available on CRAN or in the rOpenGov GitHub repo.\nWe will talk about how to join this with air pollution data and put it on the map with Milos Popovic, who prepared this nice choropleth animation.\nWe will discuss data observatories (permanent data collection programs), open collaboration (open-source inspired way of cooperation among small and large independent actors) and data altruism. Any questions: send Daniel a message on Keybase, Whatsapp or email.\nHello on International #OpenDataDay2021 from🌷 the Hague!\n- We have brought some new data to the light about 🌡climate change awareness - We created some tutorials how to harmonize survey and geographical data\n- Join us at 9.30 EST/15.30 CET 👇https://t.co/7J7pvi3sPC #ODD2021 pic.twitter.com/DwkGQaDhW1\n— dataandlyrics (@dataandlyrics) March 6, 2021 ","date":1615037400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615037400,"objectID":"ecc73affcfc1427fc7b6300d0b5e126b","permalink":"https://competition.dataobservatory.eu/talk/reprex-open-data-day-2021/","publishdate":"2021-02-03T10:10:00Z","relpermalink":"/talk/reprex-open-data-day-2021/","section":"event","summary":"Open Data Day 2021 focusing on environmental, sustainability and public spending data mapping.","tags":["open-data"],"title":"Reprex Open Data Day 2021","type":"event"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"library(regions) library(lubridate) library(dplyr) if ( dir.exists(\u0026#39;data-raw\u0026#39;) ) { data_raw_dir \u0026lt;- \u0026#34;data-raw\u0026#34; } else { data_raw_dir \u0026lt;- file.path(\u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;, \u0026#34;data-raw\u0026#34;) } The first results of our longitudinal table were difficult to map, because the surveys used an obsolete regional coding. We will adjust the wrong coding, when possible, and join the data with the European Environment Agency’s (EEA) Air Quality e-Reporting (AQ e-Reporting) data on environmental pollution. We recoded the annual level for every available reporting stations [not shown here] and all values are in μg/m3. The period under observation is 2014-2016. Data file: https://www.eea.europa.eu/data-and-maps/data/aqereporting-8 (European Environment Agency 2021).\nRecoding the Regions Recoding means that the boundaries are unchanged, but the country changed the names and codes of regions because there were other boundary changes which did not affect our observation unit. We explain the problem and the solution in greater detail in our tutorial that aggregates the data on regional levels.\npanel \u0026lt;- readRDS((file.path(data_raw_dir, \u0026#34;climate-panel.rds\u0026#34;))) climate_data_geocode \u0026lt;- panel %\u0026gt;% mutate ( year = lubridate::year(date_of_interview)) %\u0026gt;% recode_nuts() Let’s join the air pollution data and join it by corrected geocodes:\nload(file.path(\u0026#34;data\u0026#34;, \u0026#34;air_pollutants.rda\u0026#34;)) ## good practice to use system-independent file.path climate_awareness_air \u0026lt;- climate_data_geocode %\u0026gt;% rename ( region_nuts_codes = .data$code_2016) %\u0026gt;% left_join ( air_pollutants, by = \u0026#34;region_nuts_codes\u0026#34; ) %\u0026gt;% select ( -all_of(c(\u0026#34;w1\u0026#34;, \u0026#34;wex\u0026#34;, \u0026#34;date_of_interview\u0026#34;, \u0026#34;typology\u0026#34;, \u0026#34;typology_change\u0026#34;, \u0026#34;geo\u0026#34;, \u0026#34;region\u0026#34;))) %\u0026gt;% mutate ( # remove special labels and create NA_numeric_ age_education = retroharmonize::as_numeric(age_education)) %\u0026gt;% mutate_if ( is.character, as.factor) %\u0026gt;% mutate ( # we only have responses from 4 years, and this should be treated as a categorical variable year = as.factor(year) ) %\u0026gt;% filter ( complete.cases(.) ) The climate_awareness_air data frame contains the answers of 75086 individual respondents. 17.07% thought that climate change was the most serious world problem and 33.6% mentioned climate change as one of the three most important global problems.\nsummary ( climate_awareness_air ) ## rowid serious_world_problems_first ## ZA5877_v2-0-0_1 : 1 Min. :0.0000 ## ZA5877_v2-0-0_10 : 1 1st Qu.:0.0000 ## ZA5877_v2-0-0_100 : 1 Median :0.0000 ## ZA5877_v2-0-0_1000 : 1 Mean :0.1707 ## ZA5877_v2-0-0_10000: 1 3rd Qu.:0.0000 ## ZA5877_v2-0-0_10001: 1 Max. :1.0000 ## (Other) :75080 ## serious_world_problems_climate_change isocntry ## Min. :0.000 BE : 3028 ## 1st Qu.:0.000 CZ : 3023 ## Median :0.000 NL : 3019 ## Mean :0.336 SK : 3000 ## 3rd Qu.:1.000 SE : 2980 ## Max. :1.000 DE-W : 2978 ## (Other):57058 ## marital_status age_education ## (Re-)Married: without children :13242 18 :15485 ## (Re-)Married: children this marriage :12696 19 : 7728 ## Single: without children : 7650 16 : 5840 ## (Re-)Married: w children of this marriage: 6520 still studying: 5098 ## (Re-)Married: living without children : 6225 17 : 5092 ## Single: living without children : 4102 15 : 4528 ## (Other) :24651 (Other) :31315 ## age_exact occupation_of_respondent ## Min. :15.0 Retired, unable to work :22911 ## 1st Qu.:36.0 Skilled manual worker : 6774 ## Median :51.0 Employed position, at desk : 6716 ## Mean :50.1 Employed position, service job: 5624 ## 3rd Qu.:65.0 Middle management, etc. : 5252 ## Max. :99.0 Student : 5098 ## (Other) :22711 ## occupation_of_respondent_recoded ## Employed (10-18 in d15a) :32763 ## Not working (1-4 in d15a) :37125 ## Self-employed (5-9 in d15a): 5198 ## ## ## ## ## respondent_occupation_scale_c_14 ## Retired (4 in d15a) :22911 ## Manual workers (15 to 18 in d15a) :15269 ## Other white collars (13 or 14 in d15a): 9203 ## Managers (10 to 12 in d15a) : 8291 ## Self-employed (5 to 9 in d15a) : 5198 ## Students (2 in d15a) : 5098 ## (Other) : 9116 ## type_of_community is_student no_education ## DK : 34 Min. :0.0000 Min. :0.000000 ## Large town :20939 1st Qu.:0.0000 1st Qu.:0.000000 ## Rural area or village :24686 Median :0.0000 Median :0.000000 ## Small or middle sized town: 9850 Mean :0.0679 Mean :0.008151 ## Small/middle town :19577 3rd Qu.:0.0000 3rd Qu.:0.000000 ## Max. :1.0000 Max. :1.000000 ## ## education year region_nuts_codes country_code ## Min. :14.00 2013:25103 LU : 1432 DE : 4531 ## 1st Qu.:17.00 2015: 0 MT : 1398 GB : 3538 ## Median :18.00 2017:25053 CY : 1192 BE : 3028 ## Mean :19.61 2019:24930 SK02 : 1053 CZ : 3023 ## 3rd Qu.:22.00 EL30 : 974 NL : 3019 ## Max. :30.00 EE : 973 SK : 3000 ## (Other):68064 (Other):54947 ## pm2_5 pm10 o3 BaP ## Min. : 2.109 Min. : 5.883 Min. : 66.37 Min. :0.0102 ## 1st Qu.: 9.374 1st Qu.: 28.326 1st Qu.: 90.89 1st Qu.:0.1779 ## Median :11.866 Median : 33.673 Median :102.81 Median :0.4105 ## Mean :12.954 Mean : 38.637 Mean :101.49 Mean :0.8759 ## 3rd Qu.:15.890 3rd Qu.: 49.488 3rd Qu.:110.73 3rd Qu.:1.0692 ## Max. :41.293 Max. …","date":1614988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614988800,"objectID":"a62b7e3d7582ea5a29027c11a0b18178","permalink":"https://competition.dataobservatory.eu/post/2021-03-06-individual-join/","publishdate":"2021-03-06T00:00:00Z","relpermalink":"/post/2021-03-06-individual-join/","section":"post","summary":"We created a longitudinal dataset that contains data on the attitudes European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019. We join the data with air pollution data so that we can see how serious is the environmental degradation in the smaller area of each (anonymous) respondent.","tags":["retrospective-harmonization","surveys","climate-change","climate-awareness"],"title":"Where Are People More Likely To Treat Climate Change as the Most Serious Global Problem?","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"Reproducible ex post harmonization of survey microdata Retrospective survey harmonization allows the comparison of opinion poll data conducted in different countries or time. In this example we are working with data from surveys that were ex ante harmonized to a certain degree – in our tutorials we are choosing questions that were asked in the same way in many natural languages. For example, you can compare what percentage of the European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019.\nWe developed the retroharmonize R package to help this process. We have tested the package with about 80 Eurobarometer, 5 Afrobarometer survey files extensively, and a bit with Arabbarometer files. This allows the comparison of various survey answers in about 70 countries. This policy-oriented survey programs were designed to be harmonized to a certain degree, but their ex post harmonization is still necessary, challenging and errorprone. Retrospective harmonization includes harmonization of the different coding used for questions and answer options, post-stratification weights, and using different file formats.\nEurobarometer, Afrobaromer, Arab Barometer and Latinobarómetro make survey files that are harmonized across countries available for research with various terms. Our retroharmonize is not affiliated with them, and to run our examples, you must visit their websites, carefully read their terms, agree to them, and download their data yourself. What we add as a value is that we help to connect their files across time (from different years) or across these programs.\nThe survey programs mentioned above publish their data in the proprietary SPSS format. This file format can be imported and translated to R objects with the haven package; however, we needed to re-design haven’s labelled_spss class to maintain far more metadata, which, in turn, a modification of the labelled class. The haven package was designed and tested with data stored in individual SPSS files.\nThe author of labelled, Joseph Larmarange describes two main approaches to work with labelled data, such as SPSS’s method to store categorical data in the Introduction to labelled.\nTwo main approaches of labelled data conversion. Our approach is a further extension of Approach B. Survey harmonization in our case always means the joining data from several SPSS files, which requires a consistent coding among several data sources. This means that data cleaning and recoding must take place before conversion to factors, character or numeric vectors. This is particularly important with factor data (and their simple character conversions) and numeric data that occasionally contains labels, for example, to describe the reason why certain data is missing. Our tutorial vignette labelled_spss_survey gives you more information about this.\nIn the next series of tutorials, we will deal with an array of problems. These are not for the faint heart – you need to have a solid intermediate level of R to follow.\nTidy, joined survey data The original files identifiers may not be unique, we have to create new, truly unique identifiers. Weighting may not be straightforward. Neither the number of observations or the number of variables (which represents the survey questions and their translation to coded data) is the same. Certain data may be only present in one survey and not the other. This means that you will likely to run loops on lists and not data.frames, but eventually you must carefully join them. Class conversion Similar questions may be imported from a non-native R format, in our case, from an SPSS files, in an inconsistent manner. SPSS’s variable formats cannot be translated unambiguously to R classes. retroharmonize introduced a new S3 class system that handles this problem, but eventually you will have to choose if you want to see a numeric or character coding of each categorical variable. The harmonized surveys, with harmonized variable names and harmonized value labels, must be brought to consistent R representations (most statistical functions will only work on numeric, factor or character data) and carefully joined into a single data table for analysis. Harmonization of variables and variable labels Same variables may come with dissimilar variable names and variable labels. It may be a challenge to match age with age. We need to harmonize the names of variables. The harmonized variables may have different labeling. One may call refused answers as declined and the other refusal. On a simple choice, climate change may be ‘Climate change’ or Problem: Climate change. Binary choices may have survey-specific coding conventions. Value labels must be harmonized. There are good tools to do this in a single file - but we have to work with several of them. Missing value harmonization There are likely to be various types of missing values. Working with missing values is probably where most human …","date":1614816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614816000,"objectID":"1cacf8f9f78c3f525ce934648cb8eca4","permalink":"https://competition.dataobservatory.eu/post/2021-03-04_retroharmonize_intro/","publishdate":"2021-03-04T00:00:00Z","relpermalink":"/post/2021-03-04_retroharmonize_intro/","section":"post","summary":"Retrospective survey harmonization allows the comparison of opinion poll data conducted in different countries or time.  In this example we are working with data from surveys that were ex ante harmonized to a certain degree – in our tutorials we are choosing questions that were asked in the same way in many natural languages.  For example, you can compare what percentage of the European people in various countries, provinces and regions thought climate change was a serious world problem back in 2013, 2015, 2017 and 2019.","tags":["retrospective-harmonization","surveys"],"title":"What is Retrospective Survey Harmonization?","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Milos Popovic is a researcher, a data scientist, Marie Curie postdoc \u0026amp; Top 10 dataviz \u0026amp; R contributor on Twitter according to NodeXL. He took part in policy debates about terrorism and military intervention and appeared on a number of TV channels including N1 (the CNN affiliate in the Western Balkans), Serbian National Television and Al-Jazeera Balkans. My research interests are at the intersection of civil war dynamics and postwar politics in the Balkans. He is going to join the Data \u0026amp; Lyrics team on International Open Data Day to help us put harmonized environmental degradation perception and environmental sensory data on maps. We asked him four questions about his passion, mapping data. Please join us 6 March 2021 9.30 EST / 15.30 CET for an informal digital coffee.\nAs a researcher, why are you so much drawn into maps? Is this connected to your interest in territorial conflicts, or you have some other inspiration?\nThat’s a great question that really makes me pause and look back at the past 5 years. My mapping story started out of curiosity: I found interesting data on the post-WWII violence in Serbia and thought how cool it would be to make a map in R. I quickly made an unimpressive choropleth map and noticed some unexpected patterns. Then I realized just how much unused violence and census data sits out there while we have no clue about geographic patterns. So, it began. I started off with map-making but my curiosity took me to the world of georeferencing and geospatial analysis. In the process, I created over 300 maps hosted on my website as well as dozens of shapefiles from the scratch.\nI used to think that my interest is linked to growing up in a war-torn country. But, as my map-making evolved, I discovered that my passion is to use maps as a way to democratize the data: to take the scores of unused, and often buried datasets, place them on the map and share the dataviz with people.\nCan you show us an example of the best use of mapped data, and the best map that you have personally created? What is their distinctive value?\nI’m immensely proud of my work that required making the shapefiles from the scratch. For instance, my shapefile of over 1500 Kosovo cadastral settlements came into being after I turned dozens of high-resolution raster files into a shapefile fully compatible with Open Street Maps. After months of hard work, I managed to merge the shapefile with the 2011 Kosovo census and present several laser-focused demographic maps to my audience. Same goes for the settlement shapefile of Republika Srpska [the Serb-speaking entity of Bosnia-Herzegovina — the editor], which I made out of a pdf file and merged with the 2013 census data. Whereas most existing maps take a bird’s eye view, my work offers a more fine-grained view of the local dynamics to stakeholders.\nAnother similar undertaking was my transformation of the pre-WWII German military map of Yugoslavia into a unique shapefile of a few hundred Yugoslav municipalities. I combined this shapefile with the 1931 census data, 80 years after it was first published (better late than never!). It took me almost a year to complete this tremendous project but I enjoyed every bit of it. I have teamed up with my brother who is a web developer and we even made an interactive map of Yugoslavia based on the 1931 census.[The screenshot of this interactive map is the top image in the post – the editor] We hope this project would serve not only scholars but also history enthusiasts to better understand a history of the country that is no more.\nCheck out Milos’s beautiful static and interactive maps on https://milosp.info/ What do you think about collaboration based on open data and open-source software that processes such data?\nIt’s a fantastic opportunity for small teams to bypass traditional gatekeepers such as state institutions or big companies and use open source apps for the benefit of their local communities. For example, the access to Open Street Map allows small teams to map pressing communal issues as crime, deceases, or environmental degradation and come up with innovative solutions. In my work, too, I used OSM has helped me create several fine-grained maps that shed more light on local problems in Serbia such as pollution, car accidents or violence.\nWe are hoping to bring together environmental, sensory data and public attitude data on environmental issues? How can mapping help? What do you expect from this project?\nMore than ever, we are compelled to figure out how maladies spreads locally. Without mapping the hotspots, our understanding of the consequences of, for example, viral transmission or pollution is shrouded with a lot of uncertainty. We might have no clue how environmental issues shape public attitudes in localities until we use the mapping to turn on the light. Mapping would help this project pin down geographic clusters that require immediate attention from the private and public stakeholders.\nPlease join us for a digital coffee, tea …","date":1614802980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614802980,"objectID":"c54d327e4abb3f2b980e418fceacd701","permalink":"https://competition.dataobservatory.eu/post/2021-03-03-ood_interview_maps/","publishdate":"2021-03-03T22:23:00+02:00","relpermalink":"/post/2021-03-03-ood_interview_maps/","section":"post","summary":"Milos Popovic is a researcher, a data scientist, Marie Curie postdoc \u0026 Top 10 dataviz \u0026 R contributor on Twitter according to NodeXL. He is going to join the Data \u0026 Lyrics team on International Open Data Day to help us put harmonized environmental degradation perception and environmental sensory data on maps. We asked him four questions about his passion, mapping data. Please join us 6 March 2021 9.30 EST / 15.30 CET for an informal digital coffee.","tags":["open data","music-observatory","open data day","maps"],"title":"Open Data Day Interview: Mapping Data with Milos Popovic","type":"post"},{"authors":["Daniel Antal"],"categories":["R-bloggers"],"content":"In our tutorial series, we are going to harmonize the following questionnaire items from five Eurobarometer harmonized survey files. The Eurobarometer survey files are harmonized across countries, but they are only partially harmonized in time.\nAll data must be downloaded from the GESIS Data Archive in Cologne. We are not affiliated with GESIS and you must read and accept their terms to use the data.\nEurobarometer 80.2 (2013) GESIS Data Archive, Cologne. ZA5877 Data file Version 2.0.0, https://doi.org/10.4232/1.12792\nData file: ZA6595 data file (European Commission 2017). Questionnaire: Eurobarometer 83.4 Basic Bilingual Questionnaire Citation: ZA6595 Bibtex QA1a Which of the following do you consider to be the single most serious problem facing the world as a whole? (single choice)\nQA1b Which others do you consider to be serious problems? (multiple choice)\nQA2 And how serious a problem do you think climate change is at this moment? Please use a scale from 1 to 10, with \u0026#39;1\u0026#39; meaning it is \u0026#34;not at all a serious problem (scale 1-10)\nQA4 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQA4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU could benefit the EU economically (agreement-disagreement 4-scale)\nQA5 Have you personally taken any action to fight climate change over the past six months? (binary)\nEurobarometer 83.4 (2015) European Commission, Brussels; Directorate General Communication COMM.A.1 ´Strategy, Corporate Communication Actions and Eurobarometer´GESIS Data Archive, Cologne. ZA6595 Data file Version 3.0.0, https://doi.org/10.4232/1.13146\nData file: ZA6595 data file (European Commission 2018). Questionnaire: Eurobarometer 83.4 Basic Bilingual Questionnaire Citation: ZA6595 Bibtex Eurobarometer 87.1 (2017) European Commission, Brussels; Directorate General Communication, COMM.A.1 ‘Strategic Communication’; European Parliament, Directorate-General for Communication, Public Opinion Monitoring Unit GESIS Data Archive, Cologne. ZA6861 Data file Version 1.2.0, https://doi.org/10.4232/1.12922\nData file: ZA6861 data file. Questionnaire: Eurobarometer 90.2 Basic Bilingual Questionnaire Citation: ZA6861 Bibtex QC1a Which of the following do you consider to be the single most serious problem facing the world as a whole? (single choice)\nQC1b Which others do you consider to be serious problems? (multiple choice)\nQC2 And how serious a problem do you think climate change is at this moment? Please use a scale from 1 to 10, with \u0026#39;1\u0026#39; meaning it is \u0026#34;not at all a serious problem (scale 1-10)\nQc4 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Promoting EU expertise in new clean technologies to countries outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can increase the security of EU energy supplies (agreement-disagreement 4-scale)\nQc4 To what extent do you agree or disagree with each of the following statements? - More public financial support should be given to the transition to clean energies even if it means subsidies to fossil fuels should be reduced. (agreement-disagreement 4-scale)\nQc5 Have you personally taken any action to fight climate change over the past six months? (binary)\nEurobarometer 90.2 (2018) European Commission, Brussels; Directorate General Communication, COMM.A.3 ‘Media Monitoring and Eurobarometer’ GESIS Data Archive, Cologne. ZA7488 Data file Version 1.0.0, https://doi.org/10.4232/1.13289\nData file: ZA7488 data file (European Commission 2019a) Questionnaire: Eurobarometer 90.2 Basic Bilingual Questionnaire Citation: ZA7488 Bibtex QB5 To what extent do you agree or disagree with each of the following statements? - Fighting climate change and using energy more efficiently can boost the economy and jobs in the EU (agreement-disagreement 4-scale)\nQB5 To what extent do you agree or disagree with each of the following statements? - Promoting EU expertise in new clean technologies to countries outside the EU can benefit the EU economically (agreement-disagreement 4-scale)\nQB5 To what extent do you agree or disagree with each of the following statements? - Reducing fossil fuel imports from outside the EU can benefit the EU economically (agreement-disagreement …","date":1614729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614729600,"objectID":"458c154b8079191eb3ad0e229857dd05","permalink":"https://competition.dataobservatory.eu/post/2021-03-04-eurobarometer_data/","publishdate":"2021-03-03T00:00:00Z","relpermalink":"/post/2021-03-04-eurobarometer_data/","section":"post","summary":"In our [tutorial series](http://netzero.dataobservatory.eu/post/2021-03-04_retroharmonize_intro/), we are going to harmonize the following questionnaire items from five Eurobarometer harmonized survey files. The Eurobarometer survey files are harmonized across countries, but they are only partially harmonized in time.","tags":["retrospective-harmonization","surveys","eurobarometer"],"title":"Eurobarometer Surveys Used In Our Project","type":"post"},{"authors":null,"categories":null,"content":"If you cannot find the right data for your policy evaluation, your consulting project, your PhD thesis, your market research, or your scientific research project, it does not mean that the data does not exist, or that it is not available for free. In our experience, up to 95% of available open data is never used, because potential users do not realize it exists or do not know how to access it.\nEvery day, thousands of new datasets become available via the EU open data regime, freedom of information legislation in the United States and other jurisdictions, or open science and scientific reproducibility requirements — but as these datasets have been packaged or processed for different primary, original uses, they often require open data experts to locate them and adapt them to a usable form for reuse in business, scientific, or policy research.\nThe creative and cultural industries often do not participate in government statistics programs because these industries are typically comprised of microenterprises that are exempted from statistical reporting and that file only simplified financial statements and tax returns. This means that finding the appropriate private or public data sources for creative and cultural industry uses requires particularly good data maps.\nData curation means that we are continuously mapping potential data sources and sending requests to download and quality test the most current data sources. Our CEEMID project has produced several thousand indicators, of which a few dozen are available in our Demo Music Observatory.If you have specific data needs for a scientific research, policy evaluation, or business project, we can find and provide the most suitable, most current, and best value data for analysis or for ethical AI applications.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611187200,"objectID":"26cccae28579337ef19d4e2d3e6c56c6","permalink":"https://competition.dataobservatory.eu/services/data-curation/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-curation/","section":"services","summary":"We create high value key business and policy evaluation indicators. Scientific proofs require the combination of correctly matching, formatting, and verifying controlled pieces of data. Our data comes from verified and legal sources, with information about use rights and a complete history. You can always take a look at the processing code, too. We do not deal in blood diamonds.","tags":["curation"],"title":"Data Curation","type":"services"},{"authors":null,"categories":null,"content":"Data analysts spend 80% of their time on data processing, even though computers can perform these task much faster, with far less errors, and they can document the process automatically. Data processing can be shared: an analyst in a company and an analyst in an NGO does not have to reprocess the very same data twice*\nSee our blogpost How We Add Value to Public Data With Imputation and Forecasting?.\nPublic data sources are often plagued by missng values. Naively you may think that you can ignore them, but think twice: in most cases, missing data in a table is not missing information, but rather malformatted information. This approach of ignoring or dropping missing values will not be feasible or robust when you want to make a beautiful visualization, or use data in a business forecasting model, a machine learning (AI) applicaton, or a more complex scientific model. All of the above require complete datasets, and naively discarding missing data points amounts to an excessive waste of information. In this example we are continuing the example a not-so-easy to find public dataset.\nCompleting missing datapoints requires statistical production information (why might the data be missing?) and data science knowhow (how to impute the missing value.) If you do not have a good statistician or data scientist in your team, you will need high-quality, complete datasets. This is what our automated data observatories provide.\nSee our blogpost about the Data Sisyphus blogpost. We have a better solution. You can always rely on our API to import directly the latest, best data, but if you want to be sure, you can use our regular backups on Zenodo. Zenodo is an open science repository managed by CERN and supported by the European Union. On Zenodo, you can find an authoritative copy of our indicator (and its previous versions) with a digital object identifier, for example, 10.5281/zenodo.5652118. These datasets will be preserved for decades, and nobody can manipulate them. You cannot accidentally overwrite them, and we have no backdoor access to modify them.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611187200,"objectID":"d51ed70385001d6a6fc7e9ebd3da38c2","permalink":"https://competition.dataobservatory.eu/services/data-processing/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-processing/","section":"services","summary":"We create high value key business and policy evaluation indicators. Scientific proofs require the combination of correctly matching, formatting, and verifying controlled pieces of data. Our data comes from verified and legal sources, with information about use rights and a complete history. You can always take a look at the processing code, too. We do not deal in blood diamonds.","tags":["data-processing"],"title":"Data Processing","type":"services"},{"authors":null,"categories":null,"content":"We want to ensure that individual researchers, artists, and professionals, as well as NGOs and small and large organizations can benefit equally from big data in the age of artificial intelligence.\nBig data creates inequality and injustice because it is only the big corporations, big government agencies, and the biggest, best endowed universities that can finance long-lasting, comprehensive data collection programs. Big data, and large, well-processed, tidy, and accurately imputed datasets allow them to unleash the power of machine learning and AI. These large entities are able to create algorithms that decide the commercial success of your product and your artwork, giving them a competitive edge against smaller competitors while helping them evade regulations.\nCheck out our iotables software that helps the use of national accounts data from all EU members states to create economic direct, indirect and induced economic impact calculation, such as employment multipliers or GVA affects of various cultural and creative economy policies.\nCheck out our regions software that helps the harmonization of various European and African standardized surveys.\nCheck out our retroharmonize software that helps the harmonization of various European and African standardized surveys.\n","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"96b2cb649d5ffdcef063e548e1e2e92f","permalink":"https://competition.dataobservatory.eu/services/data-as-service/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/services/data-as-service/","section":"services","summary":"We provide our clients with simple datasets, databases, harmonized survey data, and various other rich data applications; we provide them with continuous access to high-quality, re-processed, re-usable public sector and scientific data.","tags":["daas","api"],"title":"Data-as-Service","type":"services"},{"authors":["Competition Data Observatory","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://competition.dataobservatory.eu/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Daniel Antal"],"categories":null,"content":"Retrospective data harmonization The aim of retroharmonize is to provide tools for reproducible retrospective (ex-post) harmonization of datasets that contain variables measuring the same concepts but coded in different ways. Ex-post data harmonization enables better use of existing data and creates new research opportunities. For example, harmonizing data from different countries enables cross-national comparisons, while merging data from different time points makes it possible to track changes over time.\nRetrospective data harmonization is associated with challenges including conceptual issues with establishing equivalence and comparability, practical complications of having to standardize the naming and coding of variables, technical difficulties with merging data stored in different formats, and the need to document a large number of data transformations. The retroharmonize package assists with the latter three components, freeing up the capacity of researchers to focus on the first.\nSpecifically, the retroharmonize package proposes a reproducible workflow, including a new class for storing data together with the harmonized and original metadata, as well as functions for importing data from different formats, harmonizing data and metadata, documenting the harmonization process, and converting between data types. See here for an overview of the functionalities.\nThe new labelled_spss_survey() class is an extension of haven’s labelled_spss class. It not only preserves variable and value labels and the user-defined missing range, but also gives an identifier, for example, the filename or the wave number, to the vector. Additionally, it enables the preservation – as metadata attributes – of the original variable names, labels, and value codes and labels, from the source data, in addition to the harmonized variable names, labels, and value codes and labels. This way, the harmonized data also contain the pre-harmonization record. The stored original metadata can be used for validation and documentation purposes.\nThe vignette Working With The labelled_spss_survey Class provides more information about the labelled_spss_survey() class.\nIn Harmonize Value Labels we discuss the characteristics of the labelled_spss_survey() class and demonstrates the problems that using this class solves.\nWe also provide three extensive case studies illustrating how the retroharmonize package can be used for ex-post harmonization of data from cross-national surveys:\nAfrobarometer Arab Barometer Eurobarometer The creators of retroharmonize are not affiliated with either Afrobarometer, Arab Barometer, Eurobarometer, or the organizations that designs, produces or archives their surveys.\nWe started building an experimental APIs data is running retroharmonize regularly and improving known statistical data sources. See: Digital Music Observatory, Green Deal Data Observatory, Economy Data Observatory.\nCitations and related work Citing the data sources Our package has been tested on three harmonized survey’s microdata. Because retroharmonize is not affiliated with any of these data sources, to replicate our tutorials or work with the data, you have download the data files from these sources, and you have to cite those sources in your work.\nAfrobarometer data: Cite Afrobarometer Arab Barometer data: cite Arab Barometer. Eurobarometer data: The Eurobarometer data Eurobarometer raw data and related documentation (questionnaires, codebooks, etc.) are made available by GESIS, ICPSR and through the Social Science Data Archive networks. You should cite your source, in our examples, we rely on the GESIS data files.\nCiting the retroharmonize R package For main developer and contributors, see the package homepage.\nThis work can be freely used, modified and distributed under the GPL-3 license:\ncitation(\u0026#34;retroharmonize\u0026#34;) #\u0026gt; #\u0026gt; To cite package \u0026#39;retroharmonize\u0026#39; in publications use: #\u0026gt; #\u0026gt; Daniel Antal (2021). retroharmonize: Ex Post Survey Data #\u0026gt; Harmonization. R package version 0.1.17. #\u0026gt; https://retroharmonize.dataobservatory.eu/ #\u0026gt; #\u0026gt; A BibTeX entry for LaTeX users is #\u0026gt; #\u0026gt; @Manual{, #\u0026gt; title = {retroharmonize: Ex Post Survey Data Harmonization}, #\u0026gt; author = {Daniel Antal}, #\u0026gt; year = {2021}, #\u0026gt; doi = {10.5281/zenodo.5006056}, #\u0026gt; note = {R package version 0.1.17}, #\u0026gt; url = {https://retroharmonize.dataobservatory.eu/}, #\u0026gt; } Contact For contact information, contributors, see the package homepage.\nCode of Conduct Please note that the retroharmonize project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624870800,"objectID":"d6f90fd73adcc2b6e69ea678f8058c1c","permalink":"https://competition.dataobservatory.eu/software/retroharmonize/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/software/retroharmonize/","section":"software","summary":"The goal of retroharmonize is to facilitate retrospective (ex-post) harmonization of data, particularly survey data, in a reproducible manner.","tags":["Surveys","Survey harmonization"],"title":"retroharmonize R package for survey harmonization","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":"iotables processes all the symmetric input-output tables of the EU member states, and calculates direct, indirect and induced effects, multipliers for GVA, employment, taxation. These are important inputs into policy evaluation, business forecasting, or granting/development indicator design. iotables is used by about 800 experts around the world.\nCode of Conduct Please note that the iotables project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\nClick the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644487200,"objectID":"e5736a65571a6c82cc15b66bbb8da8b4","permalink":"https://competition.dataobservatory.eu/software/iotables/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/software/iotables/","section":"software","summary":"The goal of iotables is to make allow a programmatic acces to the symmetric input-output tables of Eurostat. It creates multipliers, calculates direct, indirect and induced effects from European SIOT tables.","tags":["Environmental impact analysis","Economic impact analysis"],"title":"iotables R package for working with symmetric input-output tables","type":"software"},{"authors":["Daniel Antal"],"categories":null,"content":"Installation You can install the development version from GitHub with:\ndevtools::install_github(\u0026#34;rOpenGov/regions\u0026#34;) or the released version from CRAN:\ninstall.packages(\u0026#34;devtools\u0026#34;) regions currently takes care of 20,000 sub-divisional boundary changes in Europe since 1999. Comparing departments of France in 2013, with 2007 vojvodinas of Poland and 2018 megyék in Hungary? This extremely errorprone work is automated, as a result, you can compare 110-260 regions for far better analysis. regions was downloaded about 600 researchers in the first month after release.\nYou can review the complete package documentation on regions.dataobservatory.eu. If you find any problems with the code, please raise an issue on Github. Pull requests are welcome if you agree with the Contributor Code of Conduct\nIf you use regions in your work, please cite the package.\nMotivation Working with sub-national statistics has many benefits. In policymaking or in social sciences, it is a common practice to compare national statistics, which can be hugely misleading. The United States of America, the Federal Republic of Germany, Slovakia and Luxembourg are all countries, but they differ vastly in size and social homogeneity. Comparing Slovakia and Luxembourg to the federal states or even regions within Germany, or the states of Germany and the United States can provide more adequate insights. Statistically, the similarity of the aggregation level and high number of observations can allow more precise control of model parameters and errors.\nThe advantages of switching from a national level of the analysis to a sub-national level comes with a huge price in data processing, validation and imputation. The package Regions aims to help this process.\nThis package is an offspring of the eurostat package on rOpenGov. It started as a tool to validate and re-code regional Eurostat statistics, but it aims to be a general solution for all sub-national statistics. It will be developed parallel with other rOpenGov packages.\nSub-national Statistics Have Many Challenges Frequent boundary changes: as opposed to national boundaries, the territorial units, typologies are often change, and this makes the validation and recoding of observation necessary across time. For example, in the European Union, sub-national typologies change about every three years and you have to make sure that you compare the right French region in time, or, if you can make the time-wise comparison at all.\nHierarchical aggregation and special imputation: missingness is very frequent in sub-national statistics, because they are created with a serious time-lag compared to national ones, and because they are often not back-casted after boundary changes. You cannot use standard imputation algorithms because the observations are not similarly aggregated or averaged. Often, the information is seemingly missing, and it is present with an obsolete typology code.\nPackage functionality Generic vocabulary translation and joining functions for geographically coded data Keeping track of the boundary changes within the European Union between 1999-2021 Vocabulary translation and joining functions for standardized European Union statistics Vocabulary translation for the ISO-3166-2 based Google data and the European Union Imputation functions from higher aggregation hierarchy levels to lower ones, for example from NUTS1 to NUTS2 or from ISO-3166-1 to ISO-3166-2 (impute down) Imputation functions from lower hierarchy levels to higher ones (impute up) Aggregation function from lower hierarchy levels to higher ones, for example from NUTS3 to NUTS1 or from ISO-3166-2 to ISO-3166-1 (aggregate; under development) Disaggregation functions from higher hierarchy levels to lower ones, again, for example from NUTS1 to NUTS2 or from ISO-3166-1 to ISO-3166-2 (disaggregate; under development) Vignettes / Articles Working With Regional, Sub-National Statistical Products Validating Your Typology Recoding And Relabelling The Typology Of The Google Mobility Reports (COVID-19) Feedback? Raise and issue on Github or get in touch. Downloaders from CRAN: Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624276800,"objectID":"ffb0c296df8a28131e6ab084f18b3963","permalink":"https://competition.dataobservatory.eu/software/regions/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/software/regions/","section":"software","summary":"regions currently takes care of 20,000 sub-divisional boundary changes in Europe since 1999.","tags":["Regional statistics"],"title":"regions R package to create sub-national statistical indicators","type":"software"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://competition.dataobservatory.eu/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Competition Data Observatory","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://competition.dataobservatory.eu/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]